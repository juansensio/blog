{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-scoop",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/066_ssl/ssl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-april",
   "metadata": {},
   "source": [
    "# Aprendizaje Auto-Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-sharp",
   "metadata": {},
   "source": [
    "Hasta ahora hemos visto un mont칩n de ejemplos y aplicaciones de redes neuronales, y en todas ellas hemos utilizado el mismo algoritmo de aprendizaje: el aprendizaje supervisado. Este tipo de aprendizaje se caracteriza por el acceso a un conjunto de datos etiquetado, consistente en pares de ejemplos de entradas y salidas. Un ejemplo muy sencillo de entender es el entrenamiento de un clasificador de im치genes, por ejemplo usando el dataset MNIST, que est치 compuesto por un conjunto de im치genes y cada una de ellas viene acompa침ada por su correspondiente etiqueta. Esto nos permite comparar para cada imagen del dataset la salida de nuestra red con la etiqueta real, el *ground truth*, y ajustar los par치metros internos del modelo de manera iterativa para que, poco a poco, las salidas sean lo m치s parecidas a las etiquetas.\n",
    "\n",
    "Si bien este proceso es muy utilizado y ha dado muy buenos resultados en muchas aplicaciones, existen muchas otras en las que tales datasets etiquetados simplemente no existen o son escasos debido al coste y la complejidad de elaborarlos. Un ejemplo es el entrenamiento de redes neuronales para tareas de visi칩n artificial en im치genes de sat칠lite. Si queremos, por ejemplo, un detector de coches para calcular la ocupaci칩n de aparcamientos necesitaremos, primero, comprar im치genes de alta resoluci칩n (que pueden llegar a costar varios miles de euros cada una) y, segundo, etiquetar todos los coches que encontremos en las im치genes (lo cual puede llevar un buen rato). Es en este punto en el que nos preguntamos: 쯘s posible entrenar redes neuronales sin etiquetas, solo a partir de datos? A este campo se le conoce como aprendizaje no supervisado, *unsupervised learning* en ingl칠s, y es lo que vamos a explorar en este y los siguientes posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-float",
   "metadata": {},
   "source": [
    "## La analog칤a del pastel de *Yan Lecun*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-cause",
   "metadata": {},
   "source": [
    "Uno de los investigadores m치s influyentes en el 치mbito del aprendizaje no supervisado, y del mundo del aprendizaje profundo en general, es Yann Lecun. Para entender la potencia de esta forma de aprendizaje present칩 una analog칤a en la que, si el aprendizaje fuese un pastel, el aprendizaje supervisado solo corresponder칤a al recubrimiento, mientras que el aprendizaje no supervisado ser칤a el interior (y el aprendizaje por refuerzo, la cereza de arriba 游땧)\n",
    "\n",
    "![](https://miro.medium.com/max/4416/1*bvMhd_xpVxfJYoKXYp5hug.png)\n",
    "\n",
    "Esto nos da una idea de la importancia de ser capaces de entrenar nuestros modelos sin necesidad de un dataset etiquetado. Poder llevar a cabo este mecanismo de manera efectiva abrir칤a la puerta a muchas aplicaciones hoy en d칤a inconcebibles.\n",
    "\n",
    "Si bien en el campo del *machine learning* existen diferentes algoritmos no supervisados, como algoritmos de clustering (K-Means, DBSCAN, ...), estimaci칩n de probabilidad (Gaussian Mixtures) o reducci칩n de dimensionalidad (PCA y otros), los algoritmos no supervisados para *deep learning* son a d칤a de hoy un tema de investigaci칩n muy activa, siendo los algoritmos de aprendizaje auto-supervisado (*self-supervised learning*) los m치s comunes por sus buenos resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-pledge",
   "metadata": {},
   "source": [
    "## Aprendizaje Auto-Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-robertson",
   "metadata": {},
   "source": [
    "El aprendizaje auto-supervisado consiste en el entrenamiento de modelos que aprenden representaciones invariantes a distorsiones de la misma entrada. Esto significa que si un modelo es alimentado por una imagen en color y la misma imagen en blanco y negro, la representaci칩n interna (las *features* que nos dar칤a el modelo justo antes del clasificador) deber칤a ser igual, o lo m치s parecida posible. Diferentes m칠todos se basan en esta idea para construir, de una manera u otra, una funci칩n de p칠rdida que compare estas representaciones y minimize la diferencia entre pares de transformaciones que provengan de la misma imagen, mientras que maximice la diferencia entre im치genes diferentes. Algunos de los m칠todos m치s conocidos son:\n",
    "\n",
    "- Moco\n",
    "- SwAV\n",
    "- SimCLR\n",
    "\n",
    "En este post vamos a ver un ejemplo utilizando un m칠todo simple y reciente (a la hora de elaborar este post): [Barlow Twins](https://arxiv.org/abs/2103.03230). Pero antes, vamos a demostrar la necesidad de esta t칠cnica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-relation",
   "metadata": {},
   "source": [
    "## CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-failing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:24:47.271593Z",
     "start_time": "2021-05-11T10:24:47.267578Z"
    }
   },
   "source": [
    "Vamos a entrenar una red neuronal convolucional sencilla para la clasificaci칩n de im치genes con el dataset [CIFAR10](https://arxiv.org/abs/2103.03230).\n",
    "\n",
    "> El dataset CIFAR10 es muy utilizado, por lo que podr치s usarlo directamente desde `torchvision`. Sin embargo, como vamos a hacer cosas un poco m치s ex칩ticas m치s adelante, aqu칤 lo descargamos desde la fuente original y haremos nuestro propio `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "through-favor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:32:17.041486Z",
     "start_time": "2021-05-11T10:32:16.924468Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-452967de1773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
