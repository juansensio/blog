{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/041_attention/attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# Mecanismos de Atenci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el [post](https://sensioai.com/blog/040_encoder_decoder) anterior aprendimos a implementar una arquitectura de red neuronal conocida como `seq2seq`, que utiliza dos redes neuronales (el `encoder` y el `decoder`) para poder trabajar con secuencias de longitud arbitraria tanto a sus entradas como en las salidas. Este modelo nos permite llevar a cabo tareas tales como la traducci√≥n de texto entre dos idiomas, resumir un texto, responder preguntas, etc.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Si bien este modelo nos dio buenos resultados, podemos mejorarlo. Si prestamos atenci√≥n a la arquitectura que desarrollamos, el `decoder` (encargado de generar la secuencia de salida) es inicializado con el √∫ltimo estado oculto del `encoder`, el cual tiene la responsabilidad de codificar el significado de toda la frase original. Esto puede ser complicado, sobre todo al trabajar con secuencias muy largas, y para solventar este problema podemos utilizar un mecanismo de `atenci√≥n` que no solo reciba el √∫ltimo estado oculto si no tambi√©n tenga acceso a todas las salidas del `encoder` de manera que el `decoder` sea capaz de \"focalizar su atenci√≥n\" en aquellas partes m√°s importantes. Por ejemplo, para traducir la primera palabra es l√≥gico pensar que lo m√°s importante ser√° la primera palabra y sus adyacentes en la frase original, pero usar el √∫ltimo estado oculto del `encoder` puede no ser suficiente para mantener estas relaciones a largo plazo. Permitir al `decoder` acceder a esta informaci√≥n puede resultar en mejores prestaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° En la pr√°ctica, los mecanismos de atenci√≥n dan muy buenos resultados en tareas que envuelvan datos secuenciales (como aplicaciones de lenguaje). De hecho, los mejores modelos a d√≠a de hoy para tareas de `NLP` no est√°n basados en redes recurrentes sino en arquitecturas que √∫nicamente implementan mecanismos de atenci√≥n en varias capas. Estas redes neuronales son conocidas como `Transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:31:48.390280Z",
     "start_time": "2020-09-03T08:31:48.382280Z"
    }
   },
   "source": [
    "Vamos a resolver exactamente el mismo caso que en el post anterior, as√≠ que todo lo que hace referencia al procesado de datos lo dejaremos igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:28.282592Z",
     "start_time": "2020-09-04T14:01:28.264587Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.458586Z",
     "start_time": "2020-09-04T14:01:28.284587Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_file('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.474120Z",
     "start_time": "2020-09-04T14:01:31.459587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why wait until monday ?', ' por que esperar al lunes ?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.490116Z",
     "start_time": "2020-09-04T14:01:31.477120Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud m√°xima definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.506115Z",
     "start_time": "2020-09-04T14:01:31.491117Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPairs(pairs, filters, lang=0):\n",
    "    return [p for p in pairs if p[lang].startswith(filters)]\n",
    "\n",
    "def trimPairs(pairs):\n",
    "    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.407683Z",
     "start_time": "2020-09-04T14:01:31.507116Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 124547 pares de frases\n",
      "Tenemos 124124 pares de frases con longitud menor de 20\n",
      "Longitud vocabularios:\n",
      "spa 12948\n",
      "eng 25067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['can you speak french ? EOS', ' hablas frances ? EOS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(file, filters=None, reverse=False):\n",
    "    \n",
    "    pairs = read_file(file, reverse)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "    \n",
    "    if filters is not None:\n",
    "        pairs = filterPairs(pairs, filters, int(reverse))\n",
    "        print(f\"Filtramos a {len(pairs)} pares de frases\")\n",
    "        \n",
    "    pairs = trimPairs(pairs)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('spa')\n",
    "    else:\n",
    "        input_lang = Lang('spa')\n",
    "        output_lang = Lang('eng')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "        # add <eos> token\n",
    "        pair[0] += \" EOS\"\n",
    "        pair[1] += \" EOS\"\n",
    "                           \n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "                           \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('spa.txt')\n",
    "\n",
    "# descomentar para usar el dataset filtrado\n",
    "#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes)\n",
    "                           \n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.422738Z",
     "start_time": "2020-09-04T14:01:35.409686Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[204, 102, 4808, 4, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.indexesFromSentence('soy de francia . EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.438737Z",
     "start_time": "2020-09-04T14:01:35.423740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'llore', 'pruebalo', 'vete', 'EOS']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.sentenceFromIndex([8, 91, 120, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el `Dataset` nos aseguraremos de a√±adir el *padding* necesario para que todas las frases tengan la misma longitud, lo cual no hace necesario utilizar la funci√≥n `collate` que implementamos en el post anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.911736Z",
     "start_time": "2020-09-04T14:01:35.439737Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99299, 24825)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_lang, output_lang, pairs, max_length):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, ix):        \n",
    "        inputs = torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long)\n",
    "        outputs = torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n",
    "        # metemos padding a todas las frases hast a la longitud m√°xima\n",
    "        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.input_lang.word2index['PAD']), \\\n",
    "            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.output_lang.word2index['PAD'])\n",
    "\n",
    "# separamos datos en train-test\n",
    "train_size = len(pairs) * 80 // 100 \n",
    "train = pairs[:train_size]\n",
    "test = pairs[train_size:]\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(input_lang, output_lang, train, max_length=MAX_LENGTH),\n",
    "    'test': Dataset(input_lang, output_lang, test, max_length=MAX_LENGTH)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.039302Z",
     "start_time": "2020-09-04T14:01:35.912736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        device='cuda:0'),\n",
       " tensor([5, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "\n",
    "input_sentence, output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.054828Z",
     "start_time": "2020-09-04T14:01:37.040302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'],\n",
       " ['vete',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.086827Z",
     "start_time": "2020-09-04T14:01:37.055828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 20]), torch.Size([64, 20]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False),\n",
    "}\n",
    "\n",
    "inputs, outputs = next(iter(dataloader['train']))\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:13:53.670033Z",
     "start_time": "2020-09-04T08:13:53.652976Z"
    }
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La √∫nica diferencia es que, adem√°s del √∫ltimo estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.102827Z",
     "start_time": "2020-09-04T14:01:37.087831Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.134828Z",
     "start_time": "2020-09-04T14:01:37.103832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size=input_lang.n_words)\n",
    "encoder_outputs, encoder_hidden = encoder(torch.randint(0, input_lang.n_words, (64, MAX_LENGTH)))\n",
    "\n",
    "# [batch size, seq len, hidden size]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.150827Z",
     "start_time": "2020-09-04T14:01:37.135832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *decoder* con *attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de implementaci√≥n de una capa de atenci√≥n para nuestro `decoder`. En primer lugar tendremos una capa lineal que recibir√° como entradas los `embeddings` y el estado oculto anterior (concatenados). Esta capa lineal nos dar√° a la salida tantos valores como elementos tengamos en nuestras secuencias de entrada (recuerda que las hemos forzado a tener una longitud determinada). Despu√©s, aplicaremos una funci√≥n `softmax` sobre estos valores obteniendo as√≠ una distribuci√≥n de probabilidad que, seguidamente, multiplicaremos por los *outputs* del encoder (que tambi√©n tienen la misma longitud). En esta funci√≥n de probabilidad, cada elemento tiene un valor entre 0 y 1. As√≠ pues, esta operaci√≥n dar√° m√°s importancia a aquellos *outputs* del `encoder` m√°s importantes mientras que al resto les asignar√° unos valores cercanos a 0. A continuaci√≥n, concatenaremos estos valores con los `embeddings`, de nuevo, y se lo daremos a una nueva capa lineal que combinar√° estos `embeddings` con los *outputs* del `encoder` re-escalados para obtener as√≠ los *inputs* finales de la capa recurrente. \n",
    "\n",
    "En resumen, usaremos las entradas y estado oculto del `decoder` para encontrar unos pesos que re-escalar√°n las salidas del `encoder`, los cuales combinaremos de nuevo con las entradas del `decoder` para obtener las representaciones finales de nuestras frases que alimentan la capa recurrente.\n",
    "\n",
    "![](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.166827Z",
     "start_time": "2020-09-04T14:01:37.151827Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        # attention\n",
    "        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n",
    "        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_words, hidden, encoder_outputs):\n",
    "        # sacamos los embeddings\n",
    "        embedded = self.embedding(input_words)\n",
    "        # calculamos los pesos de la capa de atenci√≥n\n",
    "        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1))) \n",
    "        # re-escalamos los outputs del encoder con estos pesos\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n",
    "        # aplicamos la capa de atenci√≥n\n",
    "        output = self.attn_combine(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        # a partir de aqu√≠, como siempre. La diferencia es que la entrada a la RNN\n",
    "        # no es directmanete el embedding sino una combinaci√≥n del embedding\n",
    "        # y las salidas del encoder re-escaladas\n",
    "        output, hidden = self.gru(output.unsqueeze(1), hidden)\n",
    "        output = self.out(output.squeeze(1))        \n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.214828Z",
     "start_time": "2020-09-04T14:01:37.167830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25067])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = AttnDecoder(input_size=output_lang.n_words)\n",
    "decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, output_lang.n_words, (64, 1)), encoder_hidden, encoder_outputs)\n",
    "\n",
    "# [batch size, vocab size]\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.230826Z",
     "start_time": "2020-09-04T14:01:37.215829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.246825Z",
     "start_time": "2020-09-04T14:01:37.231826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.262825Z",
     "start_time": "2020-09-04T14:01:37.247828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch size, max_length]\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generar√° la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.278825Z",
     "start_time": "2020-09-04T14:01:37.263829Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(encoder, decoder, dataloader, epochs=10):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            input_sentences, output_sentences = batch\n",
    "            bs = input_sentences.shape[0]                    \n",
    "            loss = 0\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            # obtenemos el √∫ltimo estado oculto del encoder\n",
    "            encoder_outputs, hidden = encoder(input_sentences)\n",
    "            # calculamos las salidas del decoder de manera recurrente\n",
    "            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "            for i in range(output_sentences.shape[1]):\n",
    "                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                # el siguiente input ser√° la palabra predicha\n",
    "                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "            # optimizaci√≥n\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n",
    "            \n",
    "        val_loss = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(dataloader['test'])\n",
    "            for batch in bar:\n",
    "                input_sentences, output_sentences = batch\n",
    "                bs = input_sentences.shape[0]  \n",
    "                loss = 0\n",
    "                # obtenemos el √∫ltimo estado oculto del encoder\n",
    "                encoder_outputs, hidden = encoder(input_sentences)\n",
    "                # calculamos las salidas del decoder de manera recurrente\n",
    "                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "                for i in range(output_sentences.shape[1]):\n",
    "                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                    loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                    # el siguiente input ser√° la palabra predicha\n",
    "                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:01:26.363992Z",
     "start_time": "2020-09-04T14:01:37.280826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/1552 [00:00<?, ?it/s]C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 1/30 loss 40.64597: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.88it/s]\n",
      "Epoch 1/30 val_loss 72.90058: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.20it/s]\n",
      "Epoch 2/30 loss 32.72146: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.90it/s]\n",
      "Epoch 2/30 val_loss 70.69164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.02it/s]\n",
      "Epoch 3/30 loss 29.16241: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.90it/s]\n",
      "Epoch 3/30 val_loss 69.51955: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.28it/s]\n",
      "Epoch 4/30 loss 26.59795: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.94it/s]\n",
      "Epoch 4/30 val_loss 68.21252: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.27it/s]\n",
      "Epoch 5/30 loss 24.62131: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.91it/s]\n",
      "Epoch 5/30 val_loss 66.15660: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.26it/s]\n",
      "Epoch 6/30 loss 22.98447: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.89it/s]\n",
      "Epoch 6/30 val_loss 66.31125: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.20it/s]\n",
      "Epoch 7/30 loss 21.60687: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.78it/s]\n",
      "Epoch 7/30 val_loss 68.65877: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.10it/s]\n",
      "Epoch 8/30 loss 20.43874: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.96it/s]\n",
      "Epoch 8/30 val_loss 67.03853: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.37it/s]\n",
      "Epoch 9/30 loss 19.42336: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.94it/s]\n",
      "Epoch 9/30 val_loss 67.14857: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.65it/s]\n",
      "Epoch 10/30 loss 18.53758: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.92it/s]\n",
      "Epoch 10/30 val_loss 67.05894: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.00it/s]\n",
      "Epoch 11/30 loss 17.76296: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:50<00:00, 13.98it/s]\n",
      "Epoch 11/30 val_loss 67.14232: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.47it/s]\n",
      "Epoch 12/30 loss 17.04589: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.96it/s]\n",
      "Epoch 12/30 val_loss 67.66175: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.33it/s]\n",
      "Epoch 13/30 loss 16.43274: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.88it/s]\n",
      "Epoch 13/30 val_loss 69.09210: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.23it/s]\n",
      "Epoch 14/30 loss 15.85514: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.92it/s]\n",
      "Epoch 14/30 val_loss 69.10287: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.53it/s]\n",
      "Epoch 15/30 loss 15.35156: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:51<00:00, 13.87it/s]\n",
      "Epoch 15/30 val_loss 69.33862: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 16/30 loss 14.87437: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 16/30 val_loss 69.26039: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.11it/s]\n",
      "Epoch 17/30 loss 14.46223: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 17/30 val_loss 69.53232: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 18/30 loss 14.07377: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 18/30 val_loss 72.59765: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 19/30 loss 13.71036: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 19/30 val_loss 72.49108: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.13it/s]\n",
      "Epoch 20/30 loss 13.39023: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.76it/s]\n",
      "Epoch 20/30 val_loss 72.22702: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.15it/s]\n",
      "Epoch 21/30 loss 13.08446: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 21/30 val_loss 72.35934: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.02it/s]\n",
      "Epoch 22/30 loss 12.80340: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 22/30 val_loss 73.19710: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.15it/s]\n",
      "Epoch 23/30 loss 12.55116: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 23/30 val_loss 72.04197: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.05it/s]\n",
      "Epoch 24/30 loss 12.31468: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 24/30 val_loss 74.01761: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.08it/s]\n",
      "Epoch 25/30 loss 12.07109: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.74it/s]\n",
      "Epoch 25/30 val_loss 73.89849: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.07it/s]\n",
      "Epoch 26/30 loss 11.86944: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:53<00:00, 13.72it/s]\n",
      "Epoch 26/30 val_loss 75.09626: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.13it/s]\n",
      "Epoch 27/30 loss 11.68174: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.74it/s]\n",
      "Epoch 27/30 val_loss 74.85488: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.07it/s]\n",
      "Epoch 28/30 loss 11.49805: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:52<00:00, 13.73it/s]\n",
      "Epoch 28/30 val_loss 74.23841: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.00it/s]\n",
      "Epoch 29/30 loss 11.30296: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 29/30 val_loss 75.53842: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.04it/s]\n",
      "Epoch 30/30 loss 11.14230: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 30/30 val_loss 76.06929: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97/97 [00:07<00:00, 13.10it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(encoder, decoder, dataloader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando traducciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del ingl√©s al castellano de la siguiente manera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:21.781076Z",
     "start_time": "2020-09-04T15:43:21.769077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'],\n",
       " ['vete',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:22.157946Z",
     "start_time": "2020-09-04T15:43:22.147947Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "    # obtenemos el √∫ltimo estado oculto del encoder\n",
    "    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n",
    "    # calculamos las salidas del decoder de manera recurrente\n",
    "    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n",
    "    # iteramos hasta que el decoder nos de el token <eos>\n",
    "    outputs = []\n",
    "    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "    i = 0\n",
    "    while True:\n",
    "        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_attentions[i] = attn_weights.data\n",
    "        i += 1\n",
    "        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n",
    "        outputs.append(decoder_input.cpu().item())\n",
    "        if decoder_input.item() == output_lang.word2index['EOS']:\n",
    "            break\n",
    "    return output_lang.sentenceFromIndex(outputs), decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:22.313634Z",
     "start_time": "2020-09-04T15:43:22.302633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ve', '.', 'EOS']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attn = predict(input_sentence)\n",
    "output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de atenci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las ventajas que nos da la capa de atenci√≥n es que nos permite visualizar en qu√© partes de los inputs se fija el modelo para generar cada una de las palabras en el output, dando un grado de explicabilidad a nuestro modelo (una propiedad siempre deseada en nuestro modelos de `Machine Learning`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:23.404845Z",
     "start_time": "2020-09-04T15:43:23.397845Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    lim1, lim2 = input_sentence.index('EOS')+1, output_words.index('EOS')+1\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions[:lim2, :lim1].numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n",
    "    ax.set_yticklabels([' '] + output_words)\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:23.762833Z",
     "start_time": "2020-09-04T15:43:23.633834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showAttention(input_lang.sentenceFromIndex(input_sentence.tolist()), output_words, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como introducir mecanismos de atenci√≥n en nuestra arquitectura `encoder-decoder`, los cuales permiten a nuestra red neuronal focalizarse en partes concretas de los *inputs* a la hora de generar los *outputs*. Esta nueva capa no solo puede mejorar nuestros modelos sino que adem√°s tambi√©n es interpretable, d√°ndonos una idea del razonamiento detr√°s de las predicciones de nuestro modelo. Las redes neuronales con mejores prestaciones a d√≠a de hoy en tareas de `NLP`, los `transformers`, est√°n basados enteramente en este tipo de capas de atenci√≥n. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
