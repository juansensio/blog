{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/041_attention/attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# Mecanismos de Atenci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el [post](https://sensioai.com/blog/040_encoder_decoder) anterior aprendimos a implementar una arquitectura de red neuronal conocida como `seq2seq`, que utiliza dos redes neuronales (el `encoder` y el `decoder`) para poder trabajar con secuencias de longitud arbitraria tanto a sus entradas como en las salidas. Este modelo nos permite llevar a cabo tareas tales como la traducci√≥n de texto entre dos idiomas, resumir un texto, responder preguntas, etc.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Si bien este modelo nos dio buenos resultados, podemos mejorarlo. Si prestamos atenci√≥n a la arquitectura que desarrollamos, el `decoder` (encargado de generar la secuencia de salida) es inicializado con el √∫ltimo estado oculto del `encoder`, el cual tiene la responsabilidad de codificar el significado de toda la frase original. Esto puede ser complicado, sobre todo al trabajar con secuencias muy largas, y para solventar este problema podemos utilizar un mecanismo de `atenci√≥n` que no solo reciba el √∫ltimo estado oculto si no tambi√©n tenga acceso a todas las salidas del `encoder` de manera que el `decoder` sea capaz de \"focalizar su atenci√≥n\" en aquellas partes m√°s importantes. Por ejemplo, para traducir la primera palabra es l√≥gico pensar que lo m√°s importante ser√° la primera palabra y sus adyacentes en la frase original, pero usar el √∫ltimo estado oculto del `encoder` puede no ser suficiente para mantener estas relaciones a largo plazo. Permitir al `decoder` acceder a esta informaci√≥n puede resultar en mejores prestaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° En la pr√°ctica, los mecanismos de atenci√≥n dan muy buenos resultados en tareas que envuelvan datos secuenciales (como aplicaciones de lenguaje). De hecho, los mejores modelos a d√≠a de hoy para tareas de `NLP` no est√°n basados en redes recurrentes sino en arquitecturas que √∫nicamente implementan mecanismos de atenci√≥n en varias capas. Estas redes neuronales son conocidas como `Transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:31:48.390280Z",
     "start_time": "2020-09-03T08:31:48.382280Z"
    }
   },
   "source": [
    "Vamos a resolver exactamente el mismo caso que en el post anterior, as√≠ que todo lo que hace referencia al procesado de datos lo dejaremos igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:52.733066Z",
     "start_time": "2020-09-04T12:31:52.725066Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.056055Z",
     "start_time": "2020-09-04T12:31:52.735065Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_file('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.071561Z",
     "start_time": "2020-09-04T12:31:56.058156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graham greene is my favorite author .',\n",
       " 'graham greene es mi escritor favorito .']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.087569Z",
     "start_time": "2020-09-04T12:31:56.074570Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud m√°xima definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:56.103564Z",
     "start_time": "2020-09-04T12:31:56.088570Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPairs(pairs, filters, lang=0):\n",
    "    return [p for p in pairs if p[lang].startswith(filters)]\n",
    "\n",
    "def trimPairs(pairs):\n",
    "    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.717657Z",
     "start_time": "2020-09-04T12:31:56.104565Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 124547 pares de frases\n",
      "Tenemos 95071 pares de frases con longitud menor de 10\n",
      "Longitud vocabularios:\n",
      "spa 10881\n",
      "eng 20659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['she still loved him . EOS', 'ella aun lo amaba . EOS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(file, filters=None, reverse=False):\n",
    "    \n",
    "    pairs = read_file(file, reverse)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "    \n",
    "    if filters is not None:\n",
    "        pairs = filterPairs(pairs, filters, int(reverse))\n",
    "        print(f\"Filtramos a {len(pairs)} pares de frases\")\n",
    "        \n",
    "    pairs = trimPairs(pairs)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('spa')\n",
    "    else:\n",
    "        input_lang = Lang('spa')\n",
    "        output_lang = Lang('eng')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "        # add <eos> token\n",
    "        pair[0] += \" EOS\"\n",
    "        pair[1] += \" EOS\"\n",
    "                           \n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "                           \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('spa.txt')\n",
    "\n",
    "# descomentar para usar el dataset filtrado\n",
    "#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes)\n",
    "                           \n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.730653Z",
     "start_time": "2020-09-04T12:31:59.719659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 5028, 135, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.indexesFromSentence('tengo mucha sed .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:31:59.754653Z",
     "start_time": "2020-09-04T12:31:59.731654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ve', 'cd', 'mio', 'vete']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.sentenceFromIndex([3, 1028, 647, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el `Dataset` nos aseguraremos de a√±adir el *padding* necesario para que todas las frases tengan la misma longitud, lo cual no hace necesario utilizar la funci√≥n `collate` que implementamos en el post anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:00.233655Z",
     "start_time": "2020-09-04T12:31:59.756655Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76056, 19015)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_lang, output_lang, pairs, max_length):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, ix):        \n",
    "        inputs = torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long)\n",
    "        outputs = torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n",
    "        # metemos padding a todas las frases hast a la longitud m√°xima\n",
    "        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.input_lang.word2index['PAD']), \\\n",
    "            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.output_lang.word2index['PAD'])\n",
    "\n",
    "# separamos datos en train-test\n",
    "train_size = len(pairs) * 80 // 100 \n",
    "train = pairs[:train_size]\n",
    "test = pairs[train_size:]\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(input_lang, output_lang, train, max_length=MAX_LENGTH),\n",
    "    'test': Dataset(input_lang, output_lang, test, max_length=MAX_LENGTH)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.359235Z",
     "start_time": "2020-09-04T12:32:00.234660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 1, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'),\n",
       " tensor([5, 4, 1, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "\n",
    "input_sentence, output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.374232Z",
     "start_time": "2020-09-04T12:32:01.360239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'],\n",
       " ['vete', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.405231Z",
     "start_time": "2020-09-04T12:32:01.375236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False),\n",
    "}\n",
    "\n",
    "inputs, outputs = next(iter(dataloader['train']))\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:13:53.670033Z",
     "start_time": "2020-09-04T08:13:53.652976Z"
    }
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La √∫nica diferencia es que, adem√°s del √∫ltimo estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.421231Z",
     "start_time": "2020-09-04T12:32:01.406231Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.452231Z",
     "start_time": "2020-09-04T12:32:01.422235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size=input_lang.n_words)\n",
    "encoder_outputs, encoder_hidden = encoder(torch.randint(0, input_lang.n_words, (64, 10)))\n",
    "\n",
    "# [batch size, seq len, hidden size]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.468231Z",
     "start_time": "2020-09-04T12:32:01.453237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *decoder* con *attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de implementaci√≥n de una capa de atenci√≥n para nuestro `decoder`. En primer lugar tendremos una capa lineal que recibir√° como entradas los `embeddings` y el estado oculto anterior (concatenados). Esta capa lineal nos dar√° a la salida tantos valores como elementos tengamos en nuestras secuencias de entrada (recuerda que las hemos forzado a tener una longitud determinada). Despu√©s, aplicaremos una funci√≥n `softmax` sobre estos valores obteniendo as√≠ una distribuci√≥n de probabilidad que, seguidamente, multiplicaremos por los *outputs* del encoder (que tambi√©n tienen la misma longitud). En esta funci√≥n de probabilidad, cada elemento tiene un valor entre 0 y 1. As√≠ pues, esta operaci√≥n dar√° m√°s importancia a aquellos *outputs* del `encoder` m√°s importantes mientras que al resto les asignar√° unos valores cercanos a 0. A continuaci√≥n, concatenaremos estos valores con los `embeddings`, de nuevo, y se lo daremos a una nueva capa lineal que combinar√° estos `embeddings` con los *outputs* del `encoder` re-escalados para obtener as√≠ los *inputs* finales de la capa recurrente. \n",
    "\n",
    "En resumen, usaremos las entradas y estado oculto del `decoder` para encontrar unos pesos que re-escalar√°n las salidas del `encoder`, los cuales combinaremos de nuevo con las entradas del `decoder` para obtener las representaciones finales de nuestras frases que alimentan la capa recurrente.\n",
    "\n",
    "![](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:43:51.447608Z",
     "start_time": "2020-09-04T13:43:51.433585Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        # attention\n",
    "        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n",
    "        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_words, hidden, encoder_outputs):\n",
    "        # sacamos los embeddings\n",
    "        embedded = self.embedding(input_words)\n",
    "        # calculamos los pesos de la capa de atenci√≥n\n",
    "        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1))) \n",
    "        # re-escalamos los outputs del encoder con estos pesos\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n",
    "        # aplicamos la capa de atenci√≥n\n",
    "        output = self.attn_combine(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        # a partir de aqu√≠, como siempre. La diferencia es que la entrada a la RNN\n",
    "        # no es directmanete el embedding sino una combinaci√≥n del embedding\n",
    "        # y las salidas del encoder re-escaladas\n",
    "        output, hidden = self.gru(output.unsqueeze(1), hidden)\n",
    "        output = self.out(output.squeeze(1))        \n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.539231Z",
     "start_time": "2020-09-04T12:32:01.484236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20659])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = AttnDecoder(input_size=output_lang.n_words)\n",
    "decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, output_lang.n_words, (64, 1)), encoder_hidden, encoder_outputs)\n",
    "\n",
    "# [batch size, vocab size]\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.547232Z",
     "start_time": "2020-09-04T12:32:01.541233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.572231Z",
     "start_time": "2020-09-04T12:32:01.548231Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.578233Z",
     "start_time": "2020-09-04T12:32:01.573232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch size, max_length]\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generar√° la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:32:01.593231Z",
     "start_time": "2020-09-04T12:32:01.579232Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(encoder, decoder, dataloader, epochs=10):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            input_sentences, output_sentences = batch\n",
    "            bs = input_sentences.shape[0]                    \n",
    "            loss = 0\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            # obtenemos el √∫ltimo estado oculto del encoder\n",
    "            encoder_outputs, hidden = encoder(input_sentences)\n",
    "            # calculamos las salidas del decoder de manera recurrente\n",
    "            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "            for i in range(output_sentences.shape[1]):\n",
    "                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                # el siguiente input ser√° la palabra predicha\n",
    "                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "            # optimizaci√≥n\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n",
    "            \n",
    "        val_loss = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(dataloader['test'])\n",
    "            for batch in bar:\n",
    "                input_sentences, output_sentences = batch\n",
    "                bs = input_sentences.shape[0]  \n",
    "                loss = 0\n",
    "                # obtenemos el √∫ltimo estado oculto del encoder\n",
    "                encoder_outputs, hidden = encoder(input_sentences)\n",
    "                # calculamos las salidas del decoder de manera recurrente\n",
    "                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "                for i in range(output_sentences.shape[1]):\n",
    "                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                    loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                    # el siguiente input ser√° la palabra predicha\n",
    "                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:58:02.367102Z",
     "start_time": "2020-09-04T12:32:01.595236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/1189 [00:00<?, ?it/s]C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "Epoch 1/30 loss 34.73953: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 25.01it/s]\n",
      "Epoch 1/30 val_loss 46.46139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.29it/s]\n",
      "Epoch 2/30 loss 27.59262: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 25.13it/s]\n",
      "Epoch 2/30 val_loss 42.88374: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.24it/s]\n",
      "Epoch 3/30 loss 24.26190: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 25.17it/s]\n",
      "Epoch 3/30 val_loss 40.64831: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.23it/s]\n",
      "Epoch 4/30 loss 21.97039: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 25.00it/s]\n",
      "Epoch 4/30 val_loss 39.81519: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.11it/s]\n",
      "Epoch 5/30 loss 20.13848: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.90it/s]\n",
      "Epoch 5/30 val_loss 38.86808: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.94it/s]\n",
      "Epoch 6/30 loss 18.60935: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.91it/s]\n",
      "Epoch 6/30 val_loss 38.34785: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.98it/s]\n",
      "Epoch 7/30 loss 17.34123: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.96it/s]\n",
      "Epoch 7/30 val_loss 38.11022: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.86it/s]\n",
      "Epoch 8/30 loss 16.24881: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.97it/s]\n",
      "Epoch 8/30 val_loss 37.97426: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.94it/s]\n",
      "Epoch 9/30 loss 15.29087: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.95it/s]\n",
      "Epoch 9/30 val_loss 38.15921: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.03it/s]\n",
      "Epoch 10/30 loss 14.46075: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.95it/s]\n",
      "Epoch 10/30 val_loss 38.56259: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.95it/s]\n",
      "Epoch 11/30 loss 13.72210: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.76it/s]\n",
      "Epoch 11/30 val_loss 38.83263: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.89it/s]\n",
      "Epoch 12/30 loss 13.08170: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.82it/s]\n",
      "Epoch 12/30 val_loss 38.89201: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.93it/s]\n",
      "Epoch 13/30 loss 12.50561: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.89it/s]\n",
      "Epoch 13/30 val_loss 39.27608: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.98it/s]\n",
      "Epoch 14/30 loss 11.96744: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.87it/s]\n",
      "Epoch 14/30 val_loss 39.73010: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.99it/s]\n",
      "Epoch 15/30 loss 11.50962: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.85it/s]\n",
      "Epoch 15/30 val_loss 40.08352: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.72it/s]\n",
      "Epoch 16/30 loss 11.11470: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.87it/s]\n",
      "Epoch 16/30 val_loss 40.64367: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.66it/s]\n",
      "Epoch 17/30 loss 10.72270: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.72it/s]\n",
      "Epoch 17/30 val_loss 40.76052: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.77it/s]\n",
      "Epoch 18/30 loss 10.36965: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.82it/s]\n",
      "Epoch 18/30 val_loss 40.93134: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.86it/s]\n",
      "Epoch 19/30 loss 10.05808: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.83it/s]\n",
      "Epoch 19/30 val_loss 41.70704: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.78it/s]\n",
      "Epoch 20/30 loss 9.77267: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.80it/s]\n",
      "Epoch 20/30 val_loss 41.96183: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.85it/s]\n",
      "Epoch 21/30 loss 9.52629: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.75it/s]\n",
      "Epoch 21/30 val_loss 42.15135: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.96it/s]\n",
      "Epoch 22/30 loss 9.28994: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.78it/s]\n",
      "Epoch 22/30 val_loss 42.74523: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.66it/s]\n",
      "Epoch 23/30 loss 9.06947: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.72it/s]\n",
      "Epoch 23/30 val_loss 43.25123: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.78it/s]\n",
      "Epoch 24/30 loss 8.85993: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.77it/s]\n",
      "Epoch 24/30 val_loss 43.17854: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.83it/s]\n",
      "Epoch 25/30 loss 8.68826: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.88it/s]\n",
      "Epoch 25/30 val_loss 43.50802: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.03it/s]\n",
      "Epoch 26/30 loss 8.48245: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.78it/s]\n",
      "Epoch 26/30 val_loss 44.17614: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.86it/s]\n",
      "Epoch 27/30 loss 8.32397: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.74it/s]\n",
      "Epoch 27/30 val_loss 44.76594: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.82it/s]\n",
      "Epoch 28/30 loss 8.18032: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.76it/s]\n",
      "Epoch 28/30 val_loss 44.82165: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.96it/s]\n",
      "Epoch 29/30 loss 8.02375: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:47<00:00, 24.89it/s]\n",
      "Epoch 29/30 val_loss 44.95020: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 18.27it/s]\n",
      "Epoch 30/30 loss 7.88312: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1189/1189 [00:48<00:00, 24.76it/s]\n",
      "Epoch 30/30 val_loss 45.47675: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:04<00:00, 17.77it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(encoder, decoder, dataloader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando traducciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del ingl√©s al castellano de la siguiente manera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:31.925887Z",
     "start_time": "2020-09-04T13:12:31.915888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['really', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'],\n",
       " ['', 'en', 'serio', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][100]\n",
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:32.615887Z",
     "start_time": "2020-09-04T13:12:32.598887Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "    # obtenemos el √∫ltimo estado oculto del encoder\n",
    "    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n",
    "    # calculamos las salidas del decoder de manera recurrente\n",
    "    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n",
    "    # iteramos hasta que el decoder nos de el token <eos>\n",
    "    outputs = []\n",
    "    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "    i = 0\n",
    "    while True:\n",
    "        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_attentions[i] = attn_weights.data\n",
    "        i += 1\n",
    "        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n",
    "        outputs.append(decoder_input.cpu().item())\n",
    "        if decoder_input.item() == output_lang.word2index['EOS']:\n",
    "            break\n",
    "    return output_lang.sentenceFromIndex(outputs), decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:32.789887Z",
     "start_time": "2020-09-04T13:12:32.775888Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', 'de', 'verdad', '?', 'EOS']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attn = predict(input_sentence)\n",
    "output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de atenci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las ventajas que nos da la capa de atenci√≥n es que nos permite visualizar en qu√© partes de los inputs se fija el modelo para generar cada una de las palabras en el output, dando un grado de explicabilidad a nuestro modelo (una propiedad siempre deseada en nuestro modelos de `Machine Learning`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:33.513889Z",
     "start_time": "2020-09-04T13:12:33.505889Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    lim1, lim2 = input_sentence.index('EOS')+1, output_words.index('EOS')+1\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions[:lim2, :lim1].numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n",
    "    ax.set_yticklabels([' '] + output_words)\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:12:34.273921Z",
     "start_time": "2020-09-04T13:12:34.160888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showAttention(input_lang.sentenceFromIndex(input_sentence.tolist()), output_words, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como introducir mecanismos de atenci√≥n en nuestra arquitectura `encoder-decoder`, los cuales permiten a nuestra red neuronal focalizarse en partes concretas de los *inputs* a la hora de generar los *outputs*. Esta nueva capa no solo puede mejorar nuestros modelos sino que adem√°s tambi√©n es interpretable, d√°ndonos una idea del razonamiento detr√°s de las predicciones de nuestro modelo. Las redes neuronales con mejores prestaciones a d√≠a de hoy en tareas de `NLP`, los `transformers`, est√°n basados enteramente en este tipo de capas de atenci√≥n. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
