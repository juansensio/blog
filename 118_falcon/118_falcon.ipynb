{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/118_falcon/118_falcon.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falcon 游분"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es innegable que estamos viviendo unos tiempos muy interesantes para el desarrollo de la Inteligencia Artificial (IA), especialmente en el campo del procesamiento de lenguaje natural (NLP). Y es que parece que cada d칤a aparece un nuevo modelo de lenguaje (LLM, *Large Languaje Model*) mejor que el anterior. En este post vamos a hablar sobre [Falcon](https://falconllm.tii.ae/), el 칰ltimo de estos modelos (por lo que si est치s leyendo este post en unas semanas despu칠s de su publicaci칩n es muy probable que ya haya quedado obsoleto 游). \n",
    "\n",
    "Los responsables de Falcon son el *Technology Innovation Institute* ([TII](https://www.tii.ae/)), un centro de investigaci칩n de Abu Dhabi, y han presentado dos LLMs de diferente tama침o: [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) y [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), el primero con 7.000 millones de par치metros y el segundo con 40.000 millones. Para que te hagas una idea, GPT-3 tiene 175.000 millones de par치metros, mientras que LLaMA ofrece versiones entre los 7.000 y 65.000 millones. El principal motivo por el que Falcon est치 dando que hablar es debido a que la versi칩n de mayor tama침o sostenta actualmente las primeras posiciones en la [Open LLM Leaderdoard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) de Huggingface.\n",
    "\n",
    "![leaderboard](lb.png)\n",
    "\n",
    "En este *ranking* se eval칰an los diferentes modelos en 4 *benchmarks* populares dise침ados para evaluar las capacidades a nivel de conocimiento y razonamiento en diversos campos. 쮺omo es posible entonces que este modelo sea mejor siendo m치s \"peque침o\"? El secreto est치 en su *Dataset*.  \n",
    "\n",
    "Los grandes modelos como GPT-3 o PaLM daban m치s importancia a la cantidad de par치metros que a la cantidad de *tokens* con los que son entrenados. Sin embargo, DeepMind demostr칩 que era posible obtener resultados similares, o incluso mejores, con modelos m치s peque침os pero entrenados durante m치s tiempo, usando m치s *tokens*. De este estudio nacieron modelos como [Chinchilla](https://arxiv.org/abs/2203.15556) o los m치s actuales [LLaMA](https://arxiv.org/abs/2302.13971). En todos los casos, el texto usado para entrenar estos modelos es texto p칰blico de la web, combinado con c칩digo de Github, libros, art칤culos cient칤ficos y otras fuentes diversas. El problema es que si queremos modelos m치s capaces (con m치s par치metros), necesitamos m치s datos. Y el contenido de calidad (libros, art칤culos cient칤ficos, ...) es, por un lado, limitado y, por otro, dif칤cil de obtener y procesar. Es en este punto en el que entra en juego el dataset [RefinedWeb](https://falconllm.tii.ae/Falcon_LLM_RefinedWeb.pdf), usado para entrenar Falcon. Este dataset consiste 칰nicamente en texto extra칤do de internet, que combinado con las t칠cnicas de curado adecuadas, ha dado como lugar a un dataset de 5 trillones de *tokens*. Falcon fue entrenado en 1.5 *trillon* de estos *tokens* (por lo que a칰n hay lugar para mejora), durante 2 meses, usando 384 GPUs. En comparaci칩n, modelos como GPT-3 o PaLM no superan el trill칩n (y las estimaciones de los nuevos modelos GPT-4 o PaLM-2 hablan de 2-3 trillones). Por otro lado, LLaMA usa 1.4 trillones de *tokens*, demostrando que el uso de datos extra칤dos 칰nicamente de internet pero con el procesado adecaudo puede ser una alternativa viable a los datos de calidad, con la incre칤ble reducci칩n de coste que esto supone lo cual adem치s hace much칤simo m치s accesible el entrenamiento de LLMs a nivel de estado del arte en diferentes idiomas y dominios.\n",
    "\n",
    "![dataset](dataset.png)\n",
    "\n",
    "En cuanto al modelo, y a falta de la publicaci칩n oficial, las principales caracter칤sitcas de Falcon que conocemos son el uso de [*Flash Attention*](https://github.com/HazyResearch/flash-attention) y [Multiquery](https://arxiv.org/abs/1911.02150), dos t칠cnicas dise침adas para mejorar la eficiencia de los modelos de lenguaje durante el entrenamiento y, sobre todo, en inferencia. Esto permite superar las prestaciones de GPT-3 utilizando hasta un 75% menos de recursos computacionales.\n",
    "\n",
    "![flash](https://github.com/HazyResearch/flash-attention/raw/main/assets/flashattn_banner.jpg)\n",
    "\n",
    "Otro aspecto muy interesante es que estos modelos tienen una licencia [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) muy permisiva y que permite su uso para aplicaciones comerciales sin ning칰n tipo de restricci칩n o *royalties*. Esto tiene implicaciones muy importantes, ya que significa que cualquier empresa puede utilizar estos modelos para sus propias aplicaciones, ya sea aplicaciones internas que requieran de NLP o aplicaciones comerciales que se vendan a terceros. En el caso del uso interno tiene adem치s otra ventaja muy clara para todas aquellas aplicaciones que no quieran tener que copmpartir sus datos con sistemas externos, como la API de OpenAI por ejemplo. Esto es de especial importancia en regiones como Europa, donde la legislaci칩n de protecci칩n de datos es muy estricta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C칩mo usar Falcon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, el modelo est치 accesible de manera libre a trav칠s de [Huggingface](https://huggingface.co/tiiuae). El siguiente c칩digo descarga el modelo de lenguaje en su versi칩n peque침a para la generaci칩n de texto\n",
    "\n",
    "> Si prefieres usar el modelo grande, siemplemente usa como nombre del modelo `tiiuae/falcon-40b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11dfef8091943d3acfca7d6c68781e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 쯈ui칠n es Juan Sensio?\n",
      "Juan Sensio, un joven que hace poco abandon칩 sus estudios universitarios, y se encuentra ahora, con 28 a침os, en su primera experiencia laboral. Su pasi칩n por el dise침o y el arte lo llevaron a la Universidad de Belgrano donde estudi칩 Dise침o Industrial.\n",
      "A los pocos meses de graduarse, Juan recibi칩 una propuesta de trabajo en el departamento de Dise침o Gr치fico de una empresa del sector de los cosm칠ticos. Desde ese momento, se inici칩 su vida en el mundo laboral. Juan se hizo cargo del proyecto de una nueva l칤nea de maquillaje de los cosm칠ticos. En ese momento, se sinti칩 realizado.\n",
      "Juan se dedic칩 a su carrera en el Dise침o Industrial. Su pasi칩n por la industria del design lo llev칩 a la Universidad de Belgrano donde estudi칩\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    \"쯈ui칠n es Juan Sensio?\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi칠n han publicado versiones preparadas para chat, siguiendo la t칠cnica de aprendizaje por refuerzo con *feedback* humano (RLHF) tan popular hoy en d칤a y que ha dado lugar a los modelos que se usan en ChatGPT, entre otros. Vamos a ver un ejemplo de c칩mo usar estos modelos en la *pipeline* de [LangChain](https://python.langchain.com/en/latest/index.html) del post anterior, para chatear con un archivo PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.189'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd93e0cfe1914ee9b99367c55ca1800f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# OJO! max_length tiene que ser suficiente como para tener el documento (chuck) + el prompt + el system prompt + respuesta generada !!!\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"tiiuae/falcon-7b-instruct\", \n",
    "    task=\"text-generation\", \n",
    "    model_kwargs={\n",
    "        \"max_length\": 1024, \n",
    "        'do_sample': True,\n",
    "        'top_k': 10,\n",
    "        'num_return_sequences': 1,\n",
    "        'device_map': 'auto', \n",
    "        'trust_remote_code': True,\n",
    "        'torch_dtype': torch.bfloat16\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "Created a chunk of size 1995, which is longer than the specified 1024\n",
      "Created a chunk of size 1435, which is longer than the specified 1024\n",
      "Created a chunk of size 1167, which is longer than the specified 1024\n",
      "Created a chunk of size 1342, which is longer than the specified 1024\n",
      "Created a chunk of size 1037, which is longer than the specified 1024\n",
      "Created a chunk of size 1484, which is longer than the specified 1024\n",
      "Created a chunk of size 1057, which is longer than the specified 1024\n",
      "Created a chunk of size 1044, which is longer than the specified 1024\n",
      "Created a chunk of size 1120, which is longer than the specified 1024\n",
      "Created a chunk of size 1268, which is longer than the specified 1024\n",
      "Created a chunk of size 1464, which is longer than the specified 1024\n",
      "Created a chunk of size 1898, which is longer than the specified 1024\n",
      "Created a chunk of size 1066, which is longer than the specified 1024\n",
      "Created a chunk of size 1506, which is longer than the specified 1024\n",
      "Created a chunk of size 1270, which is longer than the specified 1024\n",
      "Created a chunk of size 1309, which is longer than the specified 1024\n",
      "Created a chunk of size 1160, which is longer than the specified 1024\n",
      "Created a chunk of size 1121, which is longer than the specified 1024\n",
      "Created a chunk of size 1060, which is longer than the specified 1024\n",
      "Created a chunk of size 1103, which is longer than the specified 1024\n",
      "Created a chunk of size 1164, which is longer than the specified 1024\n",
      "Created a chunk of size 1319, which is longer than the specified 1024\n",
      "Created a chunk of size 1148, which is longer than the specified 1024\n",
      "Created a chunk of size 1119, which is longer than the specified 1024\n",
      "Created a chunk of size 1080, which is longer than the specified 1024\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1911.01547.pdf\")\n",
    "document = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "documents = text_splitter.split_documents(document)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    vectorstore.as_retriever(), \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (1024) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The most common definition of intelligence is \"the ability to make meaningful distinctions and understand the implications of one\\'s actions.\" This involves the ability to use knowledge to achieve some goal, such as learning or problem-solving. It also includes the ability to think abstractly and apply that thinking to new situations. Some believe that intelligence can be measured, while others disagree.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What is the definition of intelligence?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos aprendido sobre el modelo de lenguaje Falcon, publicado recientemente y qu칠 ha batido a otros modelos como GPT-3, PaLM y LLaMA en los benchmarks. La principal diferencia de Falcon con respecto a estos modelos es el uso de un dataset de 5 trillones de *tokens* extra칤dos en su totalidad de internet (de los cuales solo se han usado 1.5 en su entrenamiento), que combinado con t칠cnicas de curado adecuadas, ha dado lugar a modelos con mejores prestaciones. Adem치s, el modelo est치 disponible de manera libre y con una licencia Apache 2.0, lo que permite su uso para aplicaciones comerciales sin ning칰n tipo de restricci칩n o *royalties*. 춰Ahora todo el mundo puede tener su ChatGPT sin ning칰n tipo de restricciones 游봅!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
