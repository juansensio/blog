{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/118_falcon/118_falcon.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falcon 🦅"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es innegable que estamos viviendo unos tiempos muy interesantes para el desarrollo de la Inteligencia Artificial (IA), especialmente en el campo del procesamiento de lenguaje natural (NLP). Y es que parece que cada día aparece un nuevo modelo de lenguaje (LLM, *Large Languaje Model*) mejor que el anterior. En este post vamos a hablar sobre [Falcon](https://falconllm.tii.ae/), el último de estos modelos (por lo que si estás leyendo este post en unas semanas después de su publicación es muy probable que ya haya quedado obsoleto 🥲). \n",
    "\n",
    "Los responsables de Falcon son el *Technology Innovation Institute* ([TII](https://www.tii.ae/)), un centro de investigación de Abu Dhabi, y han presentado dos LLMs de diferente tamaño: [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) y [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), el primero con 7.000 millones de parámetros y el segundo con 40.000 millones. Para que te hagas una idea, GPT-3 tiene 175.000 millones de parámetros, mientras que LLaMA ofrece versiones entre los 7.000 y 65.000 millones. El principal motivo por el que Falcon está dando que hablar es debido a que la versión de mayor tamaño sostenta actualmente las primeras posiciones en la [Open LLM Leaderdoard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) de Huggingface.\n",
    "\n",
    "![leaderboard](lb.png)\n",
    "\n",
    "En este *ranking* se evalúan los diferentes modelos en 4 *benchmarks* populares diseñados para evaluar las capacidades a nivel de conocimiento y razonamiento en diversos campos. ¿Como es posible entonces que este modelo sea mejor siendo más \"pequeño\"? El secreto está en su *Dataset*.  \n",
    "\n",
    "Los grandes modelos como GPT-3 o PaLM daban más importancia a la cantidad de parámetros que a la cantidad de *tokens* con los que son entrenados. Sin embargo, DeepMind demostró que era posible obtener resultados similares, o incluso mejores, con modelos más pequeños pero entrenados durante más tiempo, usando más *tokens*. De este estudio nacieron modelos como [Chinchilla](https://arxiv.org/abs/2203.15556) o los más actuales [LLaMA](https://arxiv.org/abs/2302.13971). En todos los casos, el texto usado para entrenar estos modelos es texto público de la web, combinado con código de Github, libros, artículos científicos y otras fuentes diversas. El problema es que si queremos modelos más capaces (con más parámetros), necesitamos más datos. Y el contenido de calidad (libros, artículos científicos, ...) es, por un lado, limitado y, por otro, difícil de obtener y procesar. Es en este punto en el que entra en juego el dataset [RefinedWeb](https://falconllm.tii.ae/Falcon_LLM_RefinedWeb.pdf), usado para entrenar Falcon. Este dataset consiste únicamente en texto extraído de internet, que combinado con las técnicas de curado adecuadas, ha dado como lugar a un dataset de 5 trillones de *tokens*. Falcon fue entrenado en 1.5 *trillon* de estos *tokens* (por lo que aún hay lugar para mejora), durante 2 meses, usando 384 GPUs. En comparación, modelos como GPT-3 o PaLM no superan el trillón (y las estimaciones de los nuevos modelos GPT-4 o PaLM-2 hablan de 2-3 trillones). Por otro lado, LLaMA usa 1.4 trillones de *tokens*, demostrando que el uso de datos extraídos únicamente de internet pero con el procesado adecaudo puede ser una alternativa viable a los datos de calidad, con la increíble reducción de coste que esto supone lo cual además hace muchísimo más accesible el entrenamiento de LLMs a nivel de estado del arte en diferentes idiomas y dominios.\n",
    "\n",
    "![dataset](dataset.png)\n",
    "\n",
    "En cuanto al modelo, y a falta de la publicación oficial, las principales caracterísitcas de Falcon que conocemos son el uso de [*Flash Attention*](https://github.com/HazyResearch/flash-attention) y [Multiquery](https://arxiv.org/abs/1911.02150), dos técnicas diseñadas para mejorar la eficiencia de los modelos de lenguaje durante el entrenamiento y, sobre todo, en inferencia. Esto permite superar las prestaciones de GPT-3 utilizando hasta un 75% menos de recursos computacionales.\n",
    "\n",
    "![flash](https://github.com/HazyResearch/flash-attention/raw/main/assets/flashattn_banner.jpg)\n",
    "\n",
    "Otro aspecto muy interesante es que estos modelos tienen una licencia [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) muy permisiva y que permite su uso para aplicaciones comerciales sin ningún tipo de restricción o *royalties*. Esto tiene implicaciones muy importantes, ya que significa que cualquier empresa puede utilizar estos modelos para sus propias aplicaciones, ya sea aplicaciones internas que requieran de NLP o aplicaciones comerciales que se vendan a terceros. En el caso del uso interno tiene además otra ventaja muy clara para todas aquellas aplicaciones que no quieran tener que copmpartir sus datos con sistemas externos, como la API de OpenAI por ejemplo. Esto es de especial importancia en regiones como Europa, donde la legislación de protección de datos es muy estricta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo usar Falcon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente, el modelo está accesible de manera libre a través de [Huggingface](https://huggingface.co/tiiuae). El siguiente código descarga el modelo de lenguaje en su versión pequeña para la generación de texto\n",
    "\n",
    "> Si prefieres usar el modelo grande, siemplemente usa como nombre del modelo `tiiuae/falcon-40b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11dfef8091943d3acfca7d6c68781e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: ¿Quién es Juan Sensio?\n",
      "Juan Sensio, un joven que hace poco abandonó sus estudios universitarios, y se encuentra ahora, con 28 años, en su primera experiencia laboral. Su pasión por el diseño y el arte lo llevaron a la Universidad de Belgrano donde estudió Diseño Industrial.\n",
      "A los pocos meses de graduarse, Juan recibió una propuesta de trabajo en el departamento de Diseño Gráfico de una empresa del sector de los cosméticos. Desde ese momento, se inició su vida en el mundo laboral. Juan se hizo cargo del proyecto de una nueva línea de maquillaje de los cosméticos. En ese momento, se sintió realizado.\n",
      "Juan se dedicó a su carrera en el Diseño Industrial. Su pasión por la industria del design lo llevó a la Universidad de Belgrano donde estudió\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    \"¿Quién es Juan Sensio?\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También han publicado versiones preparadas para chat, siguiendo la técnica de aprendizaje por refuerzo con *feedback* humano (RLHF) tan popular hoy en día y que ha dado lugar a los modelos que se usan en ChatGPT, entre otros. Vamos a ver un ejemplo de cómo usar estos modelos en la *pipeline* de [LangChain](https://python.langchain.com/en/latest/index.html) del post anterior, para chatear con un archivo PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.189'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd93e0cfe1914ee9b99367c55ca1800f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# OJO! max_length tiene que ser suficiente como para tener el documento (chuck) + el prompt + el system prompt + respuesta generada !!!\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"tiiuae/falcon-7b-instruct\", \n",
    "    task=\"text-generation\", \n",
    "    model_kwargs={\n",
    "        \"max_length\": 1024, \n",
    "        'do_sample': True,\n",
    "        'top_k': 10,\n",
    "        'num_return_sequences': 1,\n",
    "        'device_map': 'auto', \n",
    "        'trust_remote_code': True,\n",
    "        'torch_dtype': torch.bfloat16\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "Created a chunk of size 1995, which is longer than the specified 1024\n",
      "Created a chunk of size 1435, which is longer than the specified 1024\n",
      "Created a chunk of size 1167, which is longer than the specified 1024\n",
      "Created a chunk of size 1342, which is longer than the specified 1024\n",
      "Created a chunk of size 1037, which is longer than the specified 1024\n",
      "Created a chunk of size 1484, which is longer than the specified 1024\n",
      "Created a chunk of size 1057, which is longer than the specified 1024\n",
      "Created a chunk of size 1044, which is longer than the specified 1024\n",
      "Created a chunk of size 1120, which is longer than the specified 1024\n",
      "Created a chunk of size 1268, which is longer than the specified 1024\n",
      "Created a chunk of size 1464, which is longer than the specified 1024\n",
      "Created a chunk of size 1898, which is longer than the specified 1024\n",
      "Created a chunk of size 1066, which is longer than the specified 1024\n",
      "Created a chunk of size 1506, which is longer than the specified 1024\n",
      "Created a chunk of size 1270, which is longer than the specified 1024\n",
      "Created a chunk of size 1309, which is longer than the specified 1024\n",
      "Created a chunk of size 1160, which is longer than the specified 1024\n",
      "Created a chunk of size 1121, which is longer than the specified 1024\n",
      "Created a chunk of size 1060, which is longer than the specified 1024\n",
      "Created a chunk of size 1103, which is longer than the specified 1024\n",
      "Created a chunk of size 1164, which is longer than the specified 1024\n",
      "Created a chunk of size 1319, which is longer than the specified 1024\n",
      "Created a chunk of size 1148, which is longer than the specified 1024\n",
      "Created a chunk of size 1119, which is longer than the specified 1024\n",
      "Created a chunk of size 1080, which is longer than the specified 1024\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1911.01547.pdf\")\n",
    "document = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "documents = text_splitter.split_documents(document)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    vectorstore.as_retriever(), \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (1024) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The most common definition of intelligence is \"the ability to make meaningful distinctions and understand the implications of one\\'s actions.\" This involves the ability to use knowledge to achieve some goal, such as learning or problem-solving. It also includes the ability to think abstractly and apply that thinking to new situations. Some believe that intelligence can be measured, while others disagree.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What is the definition of intelligence?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos aprendido sobre el modelo de lenguaje Falcon, publicado recientemente y qué ha batido a otros modelos como GPT-3, PaLM y LLaMA en los benchmarks. La principal diferencia de Falcon con respecto a estos modelos es el uso de un dataset de 5 trillones de *tokens* extraídos en su totalidad de internet (de los cuales solo se han usado 1.5 en su entrenamiento), que combinado con técnicas de curado adecuadas, ha dado lugar a modelos con mejores prestaciones. Además, el modelo está disponible de manera libre y con una licencia Apache 2.0, lo que permite su uso para aplicaciones comerciales sin ningún tipo de restricción o *royalties*. ¡Ahora todo el mundo puede tener su ChatGPT sin ningún tipo de restricciones 🥳!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
