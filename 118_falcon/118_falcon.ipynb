{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/118_falcon/118_falcon.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falcon ü¶Ö"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es innegable que estamos viviendo unos tiempos muy interesantes para el desarrollo de la Inteligencia Artificial (IA), especialmente en el campo del procesamiento de lenguaje natural (NLP). Y es que parece que cada d√≠a aparece un nuevo modelo de lenguaje (LLM, *Large Languaje Model*) mejor que el anterior. En este post vamos a hablar sobre [Falcon](https://falconllm.tii.ae/), el √∫ltimo de estos modelos (por lo que si est√°s leyendo este post en unas semanas despu√©s de su publicaci√≥n es muy probable que ya haya quedado obsoleto ü•≤). \n",
    "\n",
    "Los responsables de Falcon son el *Technology Innovation Institute* ([TII](https://www.tii.ae/)), un centro de investigaci√≥n de Abu Dhabi, y han presentado dos LLMs de diferente tama√±o: [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) y [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), el primero con 7.000 millones de par√°metros y el segundo con 40.000 millones. Para que te hagas una idea, GPT-3 tiene 175.000 millones de par√°metros, mientras que LLaMA ofrece versiones entre los 7.000 y 65.000 millones. El principal motivo por el que Falcon est√° dando que hablar es debido a que la versi√≥n de mayor tama√±o sostenta actualmente las primeras posiciones en la [Open LLM Leaderdoard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) de Huggingface.\n",
    "\n",
    "![leaderboard](lb.png)\n",
    "\n",
    "En este *ranking* se eval√∫an los diferentes modelos en 4 *benchmarks* populares dise√±ados para evaluar las capacidades a nivel de conocimiento y razonamiento en diversos campos. ¬øComo es posible entonces que este modelo sea mejor siendo m√°s \"peque√±o\"? El secreto est√° en su *Dataset*.  \n",
    "\n",
    "Los grandes modelos como GPT-3 o PaLM daban m√°s importancia a la cantidad de par√°metros que a la cantidad de *tokens* con los que son entrenados. Sin embargo, DeepMind demostr√≥ que era posible obtener resultados similares, o incluso mejores, con modelos m√°s peque√±os pero entrenados durante m√°s tiempo, usando m√°s *tokens*. De este estudio nacieron modelos como [Chinchilla](https://arxiv.org/abs/2203.15556) o los m√°s actuales [LLaMA](https://arxiv.org/abs/2302.13971). En todos los casos, el texto usado para entrenar estos modelos es texto p√∫blico de la web, combinado con c√≥digo de Github, libros, art√≠culos cient√≠ficos y otras fuentes diversas. El problema es que si queremos modelos m√°s capaces (con m√°s par√°metros), necesitamos m√°s datos. Y el contenido de calidad (libros, art√≠culos cient√≠ficos, ...) es, por un lado, limitado y, por otro, dif√≠cil de obtener y procesar. Es en este punto en el que entra en juego el dataset [RefinedWeb](https://falconllm.tii.ae/Falcon_LLM_RefinedWeb.pdf), usado para entrenar Falcon. Este dataset consiste √∫nicamente en texto extra√≠do de internet, que combinado con las t√©cnicas de curado adecuadas, ha dado como lugar a un dataset de 5 trillones de *tokens*. Falcon fue entrenado en 1 *trillon* de estos *tokens* (por lo que a√∫n hay lugar para mejora), durante 2 meses, usando 384 GPUs. En comparaci√≥n, modelos como GPT-3 o PaLM no superan el trill√≥n (y las estimaciones de los nuevos modelos GPT-4 o PaLM-2 hablan de 2-3 trillones). Sin embargo, LLaMA usa 1.4 trillones de *tokens*, demostrando que el uso de datos extra√≠dos √∫nicamente de internet pero con el procesado adecaudo puede ser una alternativa viable a los datos de calidad, con la incre√≠ble reducci√≥n de coste que esto supone lo cual adem√°s hace much√≠simo m√°s accesible el entrenamiento de LLMs a nivel de estado del arte en diferentes idiomas y dominios.\n",
    "\n",
    "![dataset](dataset.png)\n",
    "\n",
    "En cuanto al modelo, y a falta de la publicaci√≥n oficial, las principales caracter√≠sitcas de Falcon que conocemos son el uso de [*Flash Attention*](https://github.com/HazyResearch/flash-attention) y [Multiquery](https://arxiv.org/abs/1911.02150), dos t√©cnicas dise√±adas para mejorar la eficiencia de los modelos de lenguaje durante el entrenamiento y, sobre todo, en inferencia. Esto permite superar las prestaciones de GPT-3 utilizando hasta un 75% menos de recursos computacionales.\n",
    "\n",
    "![flash](https://github.com/HazyResearch/flash-attention/raw/main/assets/flashattn_banner.jpg)\n",
    "\n",
    "Otro aspecto muy interesante es que estos modelos tienen una licencia [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) muy permisiva y que permite su uso para aplicaciones comerciales sin ning√∫n tipo de restricci√≥n o *royalties*. Esto tiene implicaciones muy importantes, ya que significa que cualquier empresa puede utilizar estos modelos para sus propias aplicaciones, ya sea aplicaciones internas que requieran de NLP o aplicaciones comerciales que se vendan a terceros. En el caso del uso interno tiene adem√°s otra ventaja muy clara para todas aquellas aplicaciones que no quieran tener que copmpartir sus datos con sistemas externos, como la API de OpenAI por ejemplo. Esto es de especial importancia en regiones como Europa, donde la legislaci√≥n de protecci√≥n de datos es muy estricta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3d778aa1ff4031976efb982685b7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: ¬øQui√©n es Juan Sensio?\n",
      "¬øQu√© es un ¬´sensio¬ª? Es una persona que vive una aventura, que est√° en un proceso de crecimiento, a la b√∫squeda de la sabidur√≠a interior que le permite ser m√°s libre, m√°s feliz, m√°s amoroso y m√°s creativo. ¬øEs Juan Sensio un psic√≥logo? No, en realidad no lo es.\n",
      "Juan no se considera ni psic√≥logo, ni psicoterapeuta. No se considera a s√≠ mismo como ¬´experto¬ª de nada. No pretende ense√±ar o transmitir ¬´conocimientos especializados¬ª. No pretende ser un maestro, pero s√≠, en cambio, un compa√±ero en el aprendizaje.\n",
      "En el mundo actual, la palabra ¬´experto¬ª es usada como una etiqueta de categor√≠a, como algo que uno ¬´ser√°¬ª y no ¬´fue¬ª (en una\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    \"¬øQui√©n es Juan Sensio?\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.189'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd9ecb1f014eaa866a3dc31503f5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# OJO! max_length tiene que ser suficiente como para tener el documento (chuck) + el prompt + el system prompt + respuesta generada !!!\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"tiiuae/falcon-7b-instruct\", \n",
    "    task=\"text-generation\", \n",
    "    model_kwargs={\n",
    "        \"max_length\": 1024, \n",
    "        'do_sample': True,\n",
    "        'top_k': 10,\n",
    "        'num_return_sequences': 1,\n",
    "        'device_map': 'auto', \n",
    "        'trust_remote_code': True,\n",
    "        'torch_dtype': torch.bfloat16\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "Created a chunk of size 1995, which is longer than the specified 1024\n",
      "Created a chunk of size 1435, which is longer than the specified 1024\n",
      "Created a chunk of size 1167, which is longer than the specified 1024\n",
      "Created a chunk of size 1342, which is longer than the specified 1024\n",
      "Created a chunk of size 1037, which is longer than the specified 1024\n",
      "Created a chunk of size 1484, which is longer than the specified 1024\n",
      "Created a chunk of size 1057, which is longer than the specified 1024\n",
      "Created a chunk of size 1044, which is longer than the specified 1024\n",
      "Created a chunk of size 1120, which is longer than the specified 1024\n",
      "Created a chunk of size 1268, which is longer than the specified 1024\n",
      "Created a chunk of size 1464, which is longer than the specified 1024\n",
      "Created a chunk of size 1898, which is longer than the specified 1024\n",
      "Created a chunk of size 1066, which is longer than the specified 1024\n",
      "Created a chunk of size 1506, which is longer than the specified 1024\n",
      "Created a chunk of size 1270, which is longer than the specified 1024\n",
      "Created a chunk of size 1309, which is longer than the specified 1024\n",
      "Created a chunk of size 1160, which is longer than the specified 1024\n",
      "Created a chunk of size 1121, which is longer than the specified 1024\n",
      "Created a chunk of size 1060, which is longer than the specified 1024\n",
      "Created a chunk of size 1103, which is longer than the specified 1024\n",
      "Created a chunk of size 1164, which is longer than the specified 1024\n",
      "Created a chunk of size 1319, which is longer than the specified 1024\n",
      "Created a chunk of size 1148, which is longer than the specified 1024\n",
      "Created a chunk of size 1119, which is longer than the specified 1024\n",
      "Created a chunk of size 1080, which is longer than the specified 1024\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1911.01547.pdf\")\n",
    "document = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "documents = text_splitter.split_documents(document)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, \n",
    "    vectorstore.as_retriever(), \n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n‚ÄúIntelligence is the ability to make thoughtful, meaningful decisions and judgements in a variety of settings.‚Äù -Robert Sternberg\\n9 1 0 2\\n. v n v o v o n 2 1 v o v o n 2 v 7 4 5 1 0. 1 1 9 1 : v i X r a x r. 1 1\\nA. 5 v 7 4 5 1.\\n1.2\\nA. o n 3 2 v n 5 2 4 1 o o n 3 v n 5 5. o o. a n 2 2 2 7. o n 7 7 7 7 7. 6 4 7 7 7.\\nA. o n x. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What is the definition of intelligence?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I.2 DeÔ¨Åning intelligence: two divergent visions\n",
      "\n",
      "Looked at in one way, everyone knows what intelligence is; looked at in another way, no one does.\n",
      "\n",
      "Robert J. Sternberg, 2000\n",
      "\n",
      "Many formal and informal deÔ¨Ånitions of intelligence have been proposed over the past few decades, although there is no existing scientiÔ¨Åc consensus around any single deÔ¨Ånition. Sternberg & Detterman noted in 1986 [87] that when two dozen prominent psychologists were asked to deÔ¨Åne intelligence, they all gave somewhat divergent answers. In the context of AI research, Legg and Hutter [53] summarized in 2007 no fewer than 70 deÔ¨Ånitions from the literature into a single statement: ‚ÄúIntelligence measures an agent‚Äôs ability to achieve goals in a wide range of environments.‚Äù\n",
      "9 1 0 2\n",
      "\n",
      "v o N 5 2\n",
      "\n",
      "] I\n",
      "\n",
      "A . s c [\n",
      "\n",
      "2 v 7 4 5 1 0 . 1 1 9 1 : v i X r a\n",
      "\n",
      "On the Measure of Intelligence\n",
      "\n",
      "Franc¬∏ois Chollet ‚àó Google, Inc. fchollet@google.com\n",
      "\n",
      "November 5, 2019\n",
      "\n",
      "Abstract\n",
      "Thus, a central point of this document is that ‚Äúgeneral intelligence‚Äù is not a binary property which a system either possesses or lacks. It is a spectrum, tied to 1) a scope of application, which may be more or less broad, and 2) the degree of efÔ¨Åciency with which the system translate its priors and experience into new skills over the scope considered, 3) the degree of generalization difÔ¨Åculty represented by different points in the scope considered (see II.2). In addition, the ‚Äúvalue‚Äù of one scope of application over another is entirely subjective; we wouldn‚Äôt be interested in (and wouldn‚Äôt even perceive as intelligent) a system whose scope of application had no intersection with our own.\n",
      "Intelligence. Cambridge University Press, 2017.\n",
      "\n",
      "[38] Jos¬¥e Hern¬¥andez-Orallo and David L Dowe. Measuring universal intelligence: To- wards an anytime intelligence test. ArtiÔ¨Åcial Intelligence, 174(18):1508‚Äì1539, 2010.\n",
      "\n",
      "[39] Jos¬¥e Hern¬¥andez-Orallo, David L. Dowe, and M.Victoria Hern¬¥andez-Lloreda. Uni-\n",
      "\n",
      "versal psychometrics. Cogn. Syst. Res., (C):50‚Äì74, March 2014.\n",
      "\n",
      "[40] Jos¬¥e Hern¬¥andez-Orallo and Neus Minaya-Collado. A formal deÔ¨Ånition of intelli-\n",
      "\n",
      "gence based on an intensional variant of algorithmic complexity. 1998.\n",
      "\n",
      "[41] G.E. Hinton. How neural networks learn from experience. Mind and brain: Read-\n",
      "\n",
      "ings from the ScientiÔ¨Åc American magazine, page 113124, 1993.\n",
      "\n",
      "[42] Thomas Hobbes. Human Nature: or The fundamental Elements of Policie. 1650.\n",
      "\n",
      "[43] Marcus Hutter. Universal artiÔ¨Åcial intelligence: Sequential decisions based on al-\n",
      "\n",
      "gorithmic probability. Springer Science & Business Media, 2004.\n",
      "\n",
      "[44] D.L. Dowe J. Hernndez-Orallo. Iq tests are not for machines, yet. Intelligence, page\n",
      "\n",
      "7781, 2012.\n"
     ]
    }
   ],
   "source": [
    "for doc in result['source_documents']:\n",
    "    print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
