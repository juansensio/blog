{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/090_dlops_onnx/090_dlops_onnx.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLOps - ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado varios modelos, los hemos comparado y hemos decidido cuál usaremos en produccion, tenemos que exportarlo. Para ello tenemos existen alternativas, en función de la aplicación (desde desplegar un modelo en dispositivos móviles o IoT hasta en servidores en la nube accesibles a través de una API). En esta serie de posts asumiermos que nuestro modelo será ejecutado en un servidor en la nube, lo cual es lo más común ya que de esta manera podemos controlar los recursos computacionales disponibles para su ejecución, monitorizarlo, desplegar nuevas versiones fácilmente, etc. En nuestro caso, que hemos entrenado los modelos usando Pytorch y Pytorch Lightning, podríamos usar cualquier *framework* en Python que nos permita servir las predicciones a través de internet, como por ejemplo [Flask](https://flask.palletsprojects.com/en/2.0.x/) o [FastAPI](https://fastapi.tiangolo.com/). El principal problema de esta opción es que tendremos que cargar todas las librerías (y sus dependencias) en nuestra API, lo cual resultará en una carga muy pesada. Recientemente, Pytorch incluye una solución dedicada para este caso de uso, [Torchserve](https://pytorch.org/serve/) que si bien nos ofrece una solución optimizada para servir modelos en producción, está limitada al uso de modelos en Pytorch.\n",
    "\n",
    "Es en este punto en el que entra [ONNX](https://onnx.ai/), un estándar abierto para la representación de redes neuronales que permite la interoperabilidad entre librerías y ofrece una solución optimizada para servir modelos en producción (tanto en la nube como on dispositvos móviles). De esta manera podemos desacoplar el entrenamiento de los modelos de su puesta en producción, utilizando en cada caso las herramientas preferidas para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar un modelo a ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como podemos exportar un modelo entrenado a ONNX. En primer lugar, cargaremos el *checkpoint* deseado (el cual generamos en el post anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import *\n",
    "\n",
    "module = MNISTModule.load_from_checkpoint('checkpoints/006-val_loss=0.14715-epoch=7.ckpt')\n",
    "module.mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena práctica evaluar nuestro modelo antes y después de exportarlo para asegurarnos de que todo funciona correctamenete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949999988079071"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "dm = MNISTDataModule(**module.hparams['datamodule'])\n",
    "dm.setup()\n",
    "\n",
    "def torch_eval():\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, labels = torch.tensor([]), torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            outputs = module.predict(imgs) > 0.5\n",
    "            preds = torch.cat([preds, outputs.cpu().long()])\n",
    "            labels = torch.cat([labels, _labels])\n",
    "\n",
    "    acc = (preds == labels).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "torch_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning nos permite exportar un modelo a ONNX de manera muy sencilla con la siguiente línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = torch.randint(0, 255, (1, 28, 28), dtype=torch.uint8)\n",
    "module.to_onnx(\n",
    "    'models/binary_classifier_3.onnx', # nombre del modelo a guardar\n",
    "    input_sample, # ejemplo de entrada\n",
    "    export_params=True, # exportar los parametros del modelo\n",
    "    opset_version=11, # en función de las ops en el modelo, se puede cambiar el opset\n",
    "    input_names = ['input'], # nombre de la entrada\tpara usar en producción\n",
    "    output_names = ['output'],  # nombre de la salida para usar en producción\n",
    "    dynamic_axes={  # para poder tener diferentes batch sizes\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'output' : {0 : 'batch_size'},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ves le tenemos que indicar el nombre del modelo y la carpeta en la que lo queremos guardar, darle unas entradas de ejemplo (que ONNX usará para ideantificar todas las operaciones que se realizan dentro del modelo y guardarlas), si queremos exportar los parámetros del modelo, la versión del `opset` (este es el conjunto de operaciones soportadas, que va cambiando a medida que se añaden nuevas funcionalidades) y, opcionalmente, nombres para entradas y salidas (esto es importante si nuestro modelo tiene varias entradas y/o salidas) así como indicar qué ejes son dinámicos (útil para poder usar el modelo en producción con diferentes *batch sizes*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNXRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo podemos cargarlo y ejecutarlo usando el *ONNXRuntime*. Esta es una de las ventajas de ONNX, y es que existen *runtimes* para multiples entornos y lenguajes. Así pues, podrás entrenar el modelo en `Python` y luego desplegarlo tanto en `Python` como en la web con `Javascript`, en dipositivos moviles con `Android` o `iOS`, etc.\n",
    "\n",
    "> En Python, puedes instalarlo con el comando `pip install onnxruntime`.\n",
    "\n",
    "Para cargar el modelo instanciamos una `InferenceSession` con el `path` al modelo exportado. Luego, definiermos las entradas al modelo usando el un `dict` con el nombre definido en la fase de exportación. Date cuenta que ahora el modelo usará como entradas array de `NumPy`, ya que en este entorno `Pytorch` ya no existe. Si que es importante, sin embargo, que uses el mismo tamaño y tipo de datos que usaste en el entrenamiento. Por último, podemos obtener las salidas del modelo usando el método `run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort \n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession('models/006.onnx')\n",
    "\n",
    "ort_inputs = {\n",
    "    \"input\": np.random.randint(0, 255, (10, 28, 28), dtype=np.uint8),\n",
    "}\n",
    "\n",
    "ort_output = ort_session.run(['output'], ort_inputs)\n",
    "ort_output[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, como comentaba antes, es importante evaluar el modelo para asegurarnos que lo hemos exportado bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def onnx_eval():\n",
    "    with torch.no_grad():\n",
    "        preds, labels = [], torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            ort_inputs = {\n",
    "                \"input\": imgs.numpy(),\n",
    "            }\n",
    "            ort_output = ort_session.run(['output'], ort_inputs)[0]\n",
    "            outputs = ort_output > 0.5\n",
    "            preds += outputs.astype(int).tolist()\n",
    "            labels = torch.cat([labels, _labels])\n",
    "    acc = (np.array(preds) == labels.numpy()).astype(float).mean()\n",
    "    return acc \n",
    "\n",
    "onnx_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso obtenemos la misma métrica de evaluación que la obtenida con el `checkpoint`, así que podemos estar tranquilos de que el modelo se comportará bien en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versionando modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo y hemos verificado que funciona bien podemos versionarlo de la misma manera que hicimos con nuestro dataset.\n",
    "\n",
    "Para ello, primero añadimos la carpeta `models` a nuestro repositorio.\n",
    "\n",
    "```\n",
    "dvc add models\n",
    "```\n",
    "\n",
    "De momento solo tenemos un modelo, el que acabamos de exportar, y podemos sincornizarlo con el repositorio remoto (en el que ya viven varias versiones de nuestro dataset) de la siguiente manera\n",
    "\n",
    "```\n",
    "dvc push\n",
    "```\n",
    "\n",
    "De esta manera, cualquier persona con accesso al proyecto en el que estamos trabajando podrá recuperar este modelo con el comando\n",
    "\n",
    "```\n",
    "dvc pull models.dvc\n",
    "```\n",
    "\n",
    "Puedes porbar que funcione eliminando la carpeta `models` y recuperándola con el comando anterior. Recuerda generar un nuevo tag y sincronizar con el repositorio de git.\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m \"primer modelo\"\n",
    "git push\n",
    "git tag -a v3 -m \"version 3\"\n",
    "git push origin --tags\n",
    "```\n",
    "\n",
    "A partir de ahora, al entrenar nuevos modelos, podemos añadrilos al repositorio con nuevo tag. Usar un modelo diferente en producción será tan sencillo como cambiar al tag adecuado, lo cual veremos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como podemos exportar nuestros modelos entrenados con `Pytorch Lightning`a `ONNX`. Esta representación intermedia nos permitirá desplegar nuestros modelos en entornos de producción gracias a la `ONNXRuntime`, la cual nos permite ejectuar el modelo de manera optimizada en multitud de entornos (Python, Web, Android, etc.). Además, hemos visto como trackear diferentes versiones de nuestro modelo con `DVC` de la misma manera que lo hacemos con nuestros datos."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74dbfc52f168b3071122cf9c0781887d6121c12f9c1b29bca56ce221bccb2a07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
