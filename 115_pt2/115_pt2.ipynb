{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/115_pt2/115_pt2.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 2.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch 2.0](https://pytorch.org/) ya est치 aqu칤 游꿀游꿀游꿀 Tras varios meses en fase beta, la segunda versi칩n de nuestro framework favorito de deep learning ya est치 disponible. Si ya sabes trabajar con Pytorch, este post te servir치 para refrescar algunos conocimientos b치sicos a la vez que aprender치s sobre las novedades de Pytorch 2.0. Por otro lado, si no sabes nada de Pytorch, este post te servir치 como introducci칩n para aprender a usarlo desde cero.\n",
    "\n",
    "> En mi canal de Yotube tengo una [lista](https://www.youtube.com/watch?v=WL50sQVdQFg&list=PLkgbkukKg_Nrk7OtpwZEdVa10LijfpyZ1) de reproducci칩n con todos los v칤deos que he grabado sobre Pytorch. Te recomiendo que le eches un vistazo si quieres aprender m치s sobre este framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 쯈u칠 es Pytorch?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` es un framework de `redes neuronales`, un conjunto de librer칤as y herramientas que nos hacen la vida m치s f치cil a la hora de dise침ar, entrenar y poner en producci칩n nuestros modelos de `Deep Learning`. Una forma sencilla de entender qu칠 es `Pytorch` es la siguiente:\n",
    "\n",
    "$$ Pytorch = Numpy + Autograd + GPU $$\n",
    "\n",
    "Quiz치s la caracter칤stica m치s relevante de Pytorch es su facilidad de uso. Esto es debido a que sigue una interfaz muy similar a la de `NumPy`, por lo que si est치s familiarizado con esta librer칤a no deber칤a costarte mucho usar `Pytorch` 游때.\n",
    "\n",
    "> Si no conces `Numpy` te recomiendo que le eches un vistazo a este [post](https://www.sensiocoders.com/blog/007_numpy).\n",
    "\n",
    "Sin embargo, la funcionalidad m치s importante que `Pytorch` ofrece es la conocidad como `autograd`, la cual nos proporciona la posibilidad de calcular derivadas de manera autom치tica con respecto a cualquier `tensor`. Esto le da a Pytorch un gran potencial para dise침ar redes neuronales complejas y entrenarlas utilizando algoritmos de descenso por gradiente sin tener que calcular todas estas derivadas manualmente. Para poder llevar a cabo estas operaciones, Pytorch va construyendo de manera din치mica un `grafo computacional`. Cada vez que aplicamos una operaci칩n sobre uno o varios tensores, 칠stos se a침aden al grafo computacional junto a la operaci칩n en concreto. De esta manera, si queremos calcular la derivada de cualquier valor con respecto a cualquier tensor, simplemente tenemos que aplicar el algoritmo de `backpropagation` (que no es m치s que la regla de la cadena de la derivada) en el grafo.\n",
    "\n",
    "Para que todo esto funcione de manera eficiente, Pytorch nos d ala posibilidad de ejecutar nuestro c칩digp en `GPU`s. Esto es posible gracias a que Pytorch est치 construido sobre `CUDA`, una librer칤a de `C++` que nos permite programar en `GPU`. Por lo tanto, si tienes una `GPU` disponible, Pytorch la utilizar치 sin pr치cticamente ning칰n cambio en tu c칩digo para acelerar los c치lculos. Si no tienes una GPU, puedes usar servicios como [Google Colab](https://colab.research.google.com/) o [Kaggle](https://www.kaggle.com/) para ejecutar tu c칩digo en la nube."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci칩n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso para empezar a trabajar con `Pytorch` es instalarlo. Para ello, puedes seguir las instrucciones que aparecen en la [p치gina oficial](https://pytorch.org/). En mi caso, voy a instalarlo usando `conda` en un ordenador con Linux y con soporte `GPU`:\n",
    "\n",
    "```bash\n",
    "conda install pytorch torchvision pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "```\n",
    "\n",
    "> Si no sabes como instalar `Python` o `Conda` en tu sistema, puedes aprender a hacerlo en este [post](https://www.sensiocoders.com/blog/001_python). Tambi칠n te recomiendo crear un entorno virtual para tu nueva instalaci칩n, as칤 evitar치s conflictos con otros proyectos que tengas en marcha. \n",
    "\n",
    "En el momento de escribir este post el comando anterior instalar치 la versi칩n de `Pytorch` 2.0, en el momento en el que tu lo hagas instalar치 la versi칩n m치s reciente hasta la fecha. Para instalar versiones diferentes vista https://pytorch.org/get-started/previous-versions/.\n",
    "\n",
    "Una vez instalado ya podr치s empezar a trabajar con `Pytorch` 游꿀游꿀游꿀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0.dev20230213+cu117'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber si la `GPU` est치 disponible, puedes ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente comando te dar치 informaci칩n sobre tu sistema (si no funciona deber치s primero instalar los drivers de `NVIDIA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 17 12:06:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:17:00.0 Off |                  N/A |\n",
      "|  0%   54C    P8    22W / 350W |   2327MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "|  0%   55C    P8    28W / 350W |      8MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1378      G   /usr/lib/xorg/Xorg                 16MiB |\n",
      "|    0   N/A  N/A      1622      G   /usr/bin/gnome-shell                8MiB |\n",
      "|    0   N/A  N/A     13407      C   ...onda3/envs/pt2/bin/python     2298MiB |\n",
      "|    1   N/A  N/A      1378      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros pasos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como comentabamos antes, `Pytorch` es muy similar a `Numpy`. Si bien el objeto principal en `Numpy` es el `array`, en `Pytorch` es el `tensor`. Un `tensor` es una matriz multidimensional con un tipo de datos concreto. Por ejemplo, podemos crear un `tensor` de 2x2 con ceros de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 2)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes crear tensores con valores aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6763, -1.5573,  0.2539])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E incluso a partir de una lista de `Python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],[4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U otro array de `Numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2],[4, 5],[5, 6]])\n",
    "x = torch.from_numpy(a)\n",
    "x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como puedes esperar, pr치cticamente todos los conceptos que ya conocemos para trabajar con `NumPy` pueden aplicarse en `Pytorch`. Esto incluye operaciones aritm칠ticas, indexado y troceado, iteraci칩n, vectorizaci칩n y broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3798,  1.3883,  2.0228],\n",
       "         [ 0.2858,  0.1163, -2.2197],\n",
       "         [ 0.0047, -0.8176, -0.7456]]),\n",
       " tensor([[ 0.0758, -1.0609,  1.0362],\n",
       "         [-1.2910, -0.9067, -0.8981],\n",
       "         [-0.6490,  0.8630, -1.0342]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operaciones\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4557,  0.3275,  3.0590],\n",
       "        [-1.0052, -0.7904, -3.1179],\n",
       "        [-0.6443,  0.0455, -1.7799]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3040,  2.4492,  0.9866],\n",
       "        [ 1.5768,  1.0230, -1.3216],\n",
       "        [ 0.6537, -1.6806,  0.2886]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3798, 1.3883, 2.0228])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexado\n",
    "\n",
    "# primera fila\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3798)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera fila, primera columna\n",
    "\n",
    "x[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3798, 1.3883, 2.0228])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primera columna\n",
    "\n",
    "x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3883,  2.0228],\n",
       "        [ 0.1163, -2.2197]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troceado\n",
    "\n",
    "x[:-1, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una funcionalidad importante del objeto `tensor` que utilizaremos muy a menudo es cambiar su forma. Esto lo conseguimos con la funci칩n `view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a침adimos una dimensi칩n extra\n",
    "\n",
    "x.view(1, 3, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estiramos en una sola dimensi칩n\n",
    "\n",
    "x.view(9).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usamos -1 para asignar todos los valores restantes a una dimensi칩n\n",
    "\n",
    "x.view(-1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos transformar un `tensor` en un `array` con la funci칩n `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37982675,  1.3883141 ,  2.0227702 ],\n",
       "       [ 0.28578275,  0.11629218, -2.2197294 ],\n",
       "       [ 0.00468784, -0.8175568 , -0.74561834]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aprender m치s sobre c칩mo funcionan estos tensores, puedes cosultar la [documentaci칩n](https://pytorch.org/docs/stable/tensors.html) y este [ejemplo](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de `autograd` en acci칩n para el c치lculo de derivadas autom치ticas. Para ello, consideremos el siguiente grafo computacional sencillo:\n",
    "\n",
    "![](https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg)\n",
    "\n",
    "Tenemos tres `tensores`, $x$, $y$ y $z$, los cuales combinamos con diferente operacion para calcular $g$. 쮺칩mo podemos encontrar la derivada de $g$ con respecto a cada uno de los tensores a la entrada?. Para el caso de $z$ esto es sencillo:\n",
    "\n",
    "$$ \\frac{dg}{dz} = p = x + y$$\n",
    "\n",
    "En el caso de $x$ y $y$ es un poco m치s complicado, ya que tenemos que aplicar la regla de la cadena de la derivada:\n",
    "\n",
    "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\frac{dp}{dx} = z $$\n",
    "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\frac{dp}{dy} = z $$\n",
    "\n",
    "Si bien en este ejemplo sencillo lo hemos podido calcular a mano, imagina tener que hacer esto en redes neuronales con miles de millones de par치metros... imposible. `Autograd` nos permite calcular estas derivadas de manera autom치tica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.tensor(2., requires_grad=True)\n",
    "p = x + y\n",
    "\n",
    "z = torch.tensor(3., requires_grad=True)\n",
    "g = p * z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello marcaremos los `tensores` de los cuales queremos calcular derivadas con la funci칩n `requires_grad`. Llamado a la funci칩n `backwerd` sobre el `tensor` de salida, `autograd` calcular치 las derivadas de manera autom치tica y las almacenar치 en el atributo `grad` de cada `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad # x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad # z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el `grafo computacional` es una herramienta extraordinaria para dise침ar `redes neuronales` de complejidad arbitraria. Con una simple funci칩n, gracias al algoritmo de `backpropagation`, podemos calcular todas las derivadas de manera sencilla (cada nodo que representa una operaci칩n solo necesita calcular su propia derivada de manera local) y optimizar el modelo con nuestro algoritmo de gradiente preferido.\n",
    "\n",
    "A침adiendo `autograd` encima de `NumPy`, `Pytorch` nos ofrece todo lo que necesitamos para dise침ar y entrenar `redes neuronales`. Puedes aprender m치s sobre `autograd` [aqu칤](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py). Sin embargo, si queremos entrenar redes muy grandes o utilizar datasets muy grandes (o ambas), el proceso de entrenamiento ser치 muy lento. Es aqu칤 donde entra en juego el 칰ltimo elemento que hace de `Pytorch` lo que es. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La 칰ltima pieza que nos falta explorar es la posibilidad de ejecutar nuestro c칩digo en `GPU`. Para ello, solo tenemos que crear nuestros `tensores` en la `GPU` y ejecutar las operaciones de la misma manera. 춰Super sencillo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0625,  0.4172, -0.3304],\n",
       "        [-1.8373,  0.2483,  0.2917],\n",
       "        [-0.0288,  2.2564, -1.1821]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3, device=\"cuda\")\n",
    "y = torch.randn(3, 3, device=\"cuda\")\n",
    "\n",
    "x * y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes son todas formas v치lidas de crear un `tensor` en la `GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0205,  0.6120,  0.1365],\n",
       "        [-0.5547, -0.8378, -0.9507],\n",
       "        [ 0.7429, -0.1749, -0.4871]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")           # device = \"cuda\" tambi칠n sirve\n",
    "\n",
    "x = torch.randn(3, 3, device=device)    # crea el tensor en la GPU\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "x = x.to(device)                        # mueve el tensor a la GPU (menos eficiente)\n",
    "x = x.cuda()                            # mueve el tensor a la GPU (menos eficiente)    \n",
    "\n",
    "device = \"cuda:0\"                       # selecciona la primera GPU, si hay m치s de una - \"cuda:1\", \"cuda:2\", etc.\n",
    "x = torch.randn(3, 3, device=device)   \n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes copiar un tensor de la `GPU` a la `CPU` con la funci칩n `cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = x.cpu()\n",
    "x = x.to(\"cpu\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente ejemplo ilustra porque es importante ejecutar nuestro c칩digo en `GPU`. En este caso, vamos a calcular el tiempo que tarda en ejecutarse la multiplicaci칩n de dos matrices grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 168 ms, total: 360 ms\n",
      "Wall time: 42.8 ms\n"
     ]
    }
   ],
   "source": [
    "# en cpu\n",
    "\n",
    "x = torch.randn(10000,10000)\n",
    "y = torch.randn(10000,10000)\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 16.6 ms, total: 17.6 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "# en gpu\n",
    "\n",
    "x = torch.randn(10000,10000).cuda()\n",
    "y = torch.randn(10000,10000).cuda()\n",
    "\n",
    "%time z = x*y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pues ahora que ya conocemos los conceptos b치sicos de `Pytorch` vamos a ver como podemos dise침ar redes neuronales. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos secuenciales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma m치s sencilla de definir una `red neuronal` en `Pytorch` es utilizando la clase `Sequentail`. Esta clase nos permite definir una secuencia de capas, que se aplicar치n de manera secuencial (las salidas de una capa ser치n la entrada de la siguiente). Vamos a definir un `Perceptr칩n Multicapa (MLP)`.\n",
    "\n",
    "> Puedes aprender m치s sobre `Perceptrones Multicapa` en este [post](https://www.sensiocoders.com/blog/023_mlp_backprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo anterior es un `MLP` con 784 entradas, una capa oculta de 100 neuronas y 10 salidas. Para *ejectuar* el modelo, podemos llamarlo como de si una funci칩n se tratase, pasando como argumento el tensor con los *inputs*.\n",
    "\n",
    "> La capa de tipo `Linear` espera un tensor de 2 dimensiones, en la cual la primera es la dimensi칩n del `batch` que puedes ser arbitraria y la segunda tiene que coincidir con el n칰mero de neuronas especificado, en nuestro ejemplo 784 en la primera capa y 100 en la segunda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(torch.randn(64, 784))\n",
    "outputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera que hemos visto antes con los tensores, podemos enviar nuestro modelo a la `GPU` para acelerar las operaciones internas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "x = torch.randn(64, 784).cuda()\n",
    "\n",
    "outputs = model(x)\n",
    "outputs.shape, outputs.device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos personalizados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien los modelos secuenciales son 칰tiles para definir redes neuronales sencillas, en la pr치ctica casi siempre necesitaremos definir redes m치s complejas. Para ello, podemos definir nuestras propias clases que hereden de la clase `Module` de `Pytorch`. Esta clase nos permite definir modelos de manera m치s flexible, ya que nos permite dise침ar la l칩gica de ejecuci칩n del modelo a nuestro gusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, D_in=784, H=100, D_out=10):\n",
    "        \n",
    "        # llamamos al constructor de la clase madre\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # definimos nuestras capas\n",
    "        self.fc1 = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    # l칩gica para calcular las salidas de la red\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, necesitamos definir una nueva clase que herede de la clase `torch.nn.Module`. Esta clase madre aportar치 toda la funcionalidad esencial que necesita una `red neuronal` (soporte GPU, iterar por sus par치meteros, etc). Luego, en esta clase necesitamos definir m칤nimos dos funciones: \n",
    "\n",
    "- `init`: en el constructor llamaremos al constructor de la clase madre y despu칠s definiremos todas las capas que querramos usar en la red.\n",
    "- `forward`: en esta funci칩n definimos toda la l칩gica que aplicaremos desde que recibimos los inputs hasta que devolvemos los outputs.\n",
    "\n",
    "En el ejemplo anterior simplemente hemos replicado la misma red (puedes conseguir el mismo efecto usando la clase `Sequential`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(784, 100, 10)\n",
    "outputs = model(torch.randn(64, 784))\n",
    "outputs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilando modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las novedades que `Pytorch 2.0` introduce es la posibilidad de `compilar` el modelo. Esto le permite *analizar* nuestro modelo para su optimizaci칩n, consiguiendo as칤 un mejor rendimiento durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compiled = torch.compile(model) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes aprender m치s sobre esta funcionalidad [aqu칤](https://pytorch.org/get-started/pytorch-2.0/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entrenar una red neuronal, necesitamos un conjunto de datos sobre el que entrenar. Para ello, `Pytorch` nos ofrece funcionalidad para su creaci칩n e iteraci칩n de manera optimizada. Vamos a ver un ejemplo usando el conjunto de datos `MNIST`, que podemos descargar usando `Scikit-Learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 28, 28), (70000,))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# mnist = fetch_openml('mnist_784', version=1)\n",
    "# X, Y = mnist[\"data\"].values.astype(float).reshape(-1, 28, 28) / 255., mnist[\"target\"].values.astype(int)\n",
    "# np.savez_compressed(\"mnist.npz\", X=X, y=Y)\n",
    "\n",
    "# la descarga puede tardar un rato, as칤 que te recomiendo comentar las l칤neas anteriores despu칠s\n",
    "# de ejecutarlas la primera vez y descomentar las siguientes para cargar los datos desde el disco\n",
    "\n",
    "X, Y = np.load(\"mnist.npz\")[\"X\"], np.load(\"mnist.npz\")[\"y\"]\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "r, c = 3, 5\n",
    "fig = plt.figure(figsize=(2*c, 2*r))\n",
    "for _r in range(r):\n",
    "    for _c in range(c):\n",
    "        plt.subplot(r, c, _r*c + _c + 1)\n",
    "        ix = random.randint(0, len(X)-1)\n",
    "        plt.imshow(X[ix], cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(Y[ix])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset consiste en im치genes de d칤gitos manuscritos (del 0 al 9) con su correspondiente etiqueta, un dataset muy 칰til para aprender a entrenar redes neuronales, en concreto para clasificaci칩n de im치genes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El `DataLoader`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `DataLoader` es un objeto que nos permite iterar nuestro dataset en `batches` de manera eficiente. Podemos pasarle como argumento cualquier iterador, desde una lista de `Python` hasta un `array` de `NumPy` o un `tensor` de `Pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(X, batch_size=100)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene varias opciones interesantes que nos permitir치n mejorar la eficiencia de nuestro entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    X,                      # datos                 \n",
    "    batch_size=100,         # tama침o del batch, n칰mero de im치genes por iteraci칩n\n",
    "    shuffle=True,           # barajamos los datos antes de cada epoch\n",
    "    num_workers=4,          # n칰mero de procesos que se lanzan para cargar los datos (n칰mero de cores de la CPU para carga en paralelo)\n",
    "    pin_memory=True,        # si tenemos una GPU, los datos se cargan en la memoria de la GPU\n",
    "    collate_fn=None,        # funci칩n para combinar los datos de cada batch\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El `Dataset` "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien el `dataloader` es capaz de trabajar con cualquier iterador, `Pytorch` nos ofrece una clase base para crear nuestros propios datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # constructor\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.Y = torch.tensor(Y).long()\n",
    "    # cantidad de muestras en el dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    # devolvemos el elemento `ix` del dataset\n",
    "    def __getitem__(self, ix):\n",
    "        return self.X[ix], self.Y[ix]\n",
    "    # opcionalmente, podemos definir una funci칩n para generar un batch\n",
    "    def collate_fn(self, batch):\n",
    "        x, y = [], []\n",
    "        for _x, _y in batch:\n",
    "            x.append(_x)\n",
    "            y.append(_y)\n",
    "        return torch.stack(x).view(len(batch), -1), torch.stack(y) # estiramos las im치genes en una dimensi칩n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello crearemos una nueva clase que hereda de `torch.utils.data.Dataset`, en la cual definiremos estas tres funciones: \n",
    "\n",
    "- `__init__`: el constructor\n",
    "- `__len__`: devuelve el n칰mero de muestras en el dataset\n",
    "- `__getitem__`: devuelve una muestra en concreto del dataset\n",
    "\n",
    "Cada vez que nuestro `dataloader` necesite una nueva muestra, llamar치 a la funci칩n `__getitem__` pas치ndole el 칤ndice de la muestra que necesita. Aqu칤 podremos definir cualquier l칩gica de carga y procesado de datos (por ejemplo, leer im치genes y aplicar transformaciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(X, Y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, collate_fn=dataset.collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha llegado el momento de ver c칩mo podemos entrenar nuestro modelo con el dataset que hemos creado. Empezaremos con un ejemplo m칤nimo, en el que entrenaremos nuestro `MLP` con el dataset `MNIST`.\n",
    "\n",
    "![](https://media.licdn.com/dms/image/C4D12AQFKRQOp_aXz0g/article-cover_image-shrink_600_2000/0/1577211633664?e=2147483647&v=beta&t=pZ5TdXalXICIqPXTDBX6NP-CYlGS3wE2Kn6y1XiYjHI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem치s del modelo y los datos necesitaremos dos elementos m치s para el enterenamiento:\n",
    "\n",
    "- Una funci칩n de p칠rdida (medir치 el error del modelo)\n",
    "- Un optimizador (se encargar치 de actualizar los par치metros del modelo para minimizar la funci칩n de p칠rdida)\n",
    "\n",
    "En ambos casos, `Pytorch` nos ofrece una amplia gama de opciones, que podemos consultar en la [documentaci칩n](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
    "\n",
    "> Puedes aprender m치s sobre estos conceptos [aqu칤](https://www.sensiocoders.com/blog/013_perceptron2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5\n",
      "loss: 2.3185 [  100/70000]\n",
      "loss: 0.4671 [10100/70000]\n",
      "loss: 0.4740 [20100/70000]\n",
      "loss: 0.3378 [30100/70000]\n",
      "loss: 0.2511 [40100/70000]\n",
      "loss: 0.2768 [50100/70000]\n",
      "loss: 0.1458 [60100/70000]\n",
      "epoch: 2/5\n",
      "loss: 0.1585 [  100/70000]\n",
      "loss: 0.2071 [10100/70000]\n",
      "loss: 0.2558 [20100/70000]\n",
      "loss: 0.2064 [30100/70000]\n",
      "loss: 0.1526 [40100/70000]\n",
      "loss: 0.1505 [50100/70000]\n",
      "loss: 0.0805 [60100/70000]\n",
      "epoch: 3/5\n",
      "loss: 0.0998 [  100/70000]\n",
      "loss: 0.1444 [10100/70000]\n",
      "loss: 0.1918 [20100/70000]\n",
      "loss: 0.1645 [30100/70000]\n",
      "loss: 0.1111 [40100/70000]\n",
      "loss: 0.1036 [50100/70000]\n",
      "loss: 0.0499 [60100/70000]\n",
      "epoch: 4/5\n",
      "loss: 0.0772 [  100/70000]\n",
      "loss: 0.1111 [10100/70000]\n",
      "loss: 0.1472 [20100/70000]\n",
      "loss: 0.1387 [30100/70000]\n",
      "loss: 0.0852 [40100/70000]\n",
      "loss: 0.0851 [50100/70000]\n",
      "loss: 0.0282 [60100/70000]\n",
      "epoch: 5/5\n",
      "loss: 0.0642 [  100/70000]\n",
      "loss: 0.0917 [10100/70000]\n",
      "loss: 0.1263 [20100/70000]\n",
      "loss: 0.1171 [30100/70000]\n",
      "loss: 0.0722 [40100/70000]\n",
      "loss: 0.0764 [50100/70000]\n",
      "loss: 0.0187 [60100/70000]\n"
     ]
    }
   ],
   "source": [
    "# instanciamos nuestro dataset\n",
    "dataset = Dataset(X, Y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, collate_fn=dataset.collate_fn)\n",
    "# instanciamos nuestro modelo\n",
    "model = Model(784, 100, 10)\n",
    "# definimos la funci칩n de p칠rdida y el optimizador\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# bucle de entrenamiento\n",
    "epochs = 5\n",
    "for e in range(1, epochs+1):\n",
    "    print(f\"epoch: {e}/{epochs}\")\n",
    "    for batch_ix, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()           # reseteamos los gradientes\n",
    "        outputs = model(x)              # calculamos las salidas\n",
    "        loss = criterion(outputs, y)    # calculamos la p칠rdida\n",
    "        loss.backward()                 # calculamos los gradientes\n",
    "        optimizer.step()                # actualizamos los par치metros\n",
    "        if batch_ix % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_ix + 1) * len(x)\n",
    "            print(f\"loss: {loss:.4f} [{current:>5d}/{len(dataset):>5d}]\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si todo va seg칰n lo planeado, deber칤amos ver como la funci칩n de p칠rdida va disminuyendo a medida que el modelo va aprendiendo. En un ejemplo real, sin embargo, haremos un entrenamiento m치s sofisticado, en el que dividiremos nuestro dataset en dos partes: una para entrenar y otra para validar el modelo; y tambi칠n trackearemos diversas m칠tricas para evaluar el rendimiento del modelo (aunque en un caso real deber칤a usar algun sistema de trackeado como `Weights and Biases` o `MLFLow`).\n",
    "\n",
    "> Puedes aprender m치s sobre este tema [aqu칤](https://www.sensiocoders.com/blog/030_data_splitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5\n",
      "loss: 2.3281 [  100/60000]\n",
      "loss: 0.4712 [10100/60000]\n",
      "loss: 0.4959 [20100/60000]\n",
      "loss: 0.3392 [30100/60000]\n",
      "loss: 0.2554 [40100/60000]\n",
      "loss: 0.2822 [50100/60000]\n",
      "val_loss: 0.2487 val_acc: 0.9275\n",
      "epoch: 2/5\n",
      "loss: 0.2017 [  100/60000]\n",
      "loss: 0.2150 [10100/60000]\n",
      "loss: 0.2756 [20100/60000]\n",
      "loss: 0.2219 [30100/60000]\n",
      "loss: 0.1550 [40100/60000]\n",
      "loss: 0.1612 [50100/60000]\n",
      "val_loss: 0.1753 val_acc: 0.9474\n",
      "epoch: 3/5\n",
      "loss: 0.1222 [  100/60000]\n",
      "loss: 0.1526 [10100/60000]\n",
      "loss: 0.1959 [20100/60000]\n",
      "loss: 0.1804 [30100/60000]\n",
      "loss: 0.1038 [40100/60000]\n",
      "loss: 0.1215 [50100/60000]\n",
      "val_loss: 0.1395 val_acc: 0.9580\n",
      "epoch: 4/5\n",
      "loss: 0.0852 [  100/60000]\n",
      "loss: 0.1147 [10100/60000]\n",
      "loss: 0.1590 [20100/60000]\n",
      "loss: 0.1604 [30100/60000]\n",
      "loss: 0.0801 [40100/60000]\n",
      "loss: 0.1088 [50100/60000]\n",
      "val_loss: 0.1198 val_acc: 0.9637\n",
      "epoch: 5/5\n",
      "loss: 0.0675 [  100/60000]\n",
      "loss: 0.0874 [10100/60000]\n",
      "loss: 0.1338 [20100/60000]\n",
      "loss: 0.1493 [30100/60000]\n",
      "loss: 0.0692 [40100/60000]\n",
      "loss: 0.1007 [50100/60000]\n",
      "val_loss: 0.1090 val_acc: 0.9664\n"
     ]
    }
   ],
   "source": [
    "# instanciamos nuestro dataset\n",
    "dataset = {\n",
    "    \"train\": Dataset(X[:60000], Y[:60000]),\n",
    "    \"val\": Dataset(X[60000:], Y[60000:])\n",
    "}\n",
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=100, collate_fn=dataset['train'].collate_fn),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=100, collate_fn=dataset['val'].collate_fn)\n",
    "}\n",
    "# instanciamos nuestro modelo\n",
    "model = Model(784, 100, 10)\n",
    "# definimos la funci칩n de p칠rdida y el optimizador\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# bucle de entrenamiento\n",
    "epochs = 5\n",
    "for e in range(1, epochs+1):\n",
    "    print(f\"epoch: {e}/{epochs}\")\n",
    "    # entrenamiento\n",
    "    model.train()\n",
    "    for batch_ix, (x, y) in enumerate(dataloader['train']):\n",
    "        optimizer.zero_grad()           \n",
    "        outputs = model(x)              \n",
    "        loss = criterion(outputs, y)    \n",
    "        loss.backward()                 \n",
    "        optimizer.step()                \n",
    "        if batch_ix % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_ix + 1) * len(x)\n",
    "            print(f\"loss: {loss:.4f} [{current:>5d}/{len(dataset['train']):>5d}]\")\n",
    "    # validaci칩n\n",
    "    model.eval()                \n",
    "    val_loss, val_acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_ix, (x, y) in enumerate(dataloader['val']):\n",
    "            outputs = model(x)              \n",
    "            loss = criterion(outputs, y)    \n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append((outputs.argmax(1) == y).float().mean().item())\n",
    "    print(f\"val_loss: {np.mean(val_loss):.4f} val_acc: {np.mean(val_acc):.4f}\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar podemos poner el modelo en modo entrenamiento o evaluaci칩n con las funciones `train` y `eval`. Esto es importante ya que algunas capas (como `Dropout` o `BatchNorm`) tienen comportamientos diferentes en cada modo. Adem치s, durante la validaci칩n, usamos el contexto `torch.no_grad` para que no se calcule el gradiente, ya que no lo necesitamos (esto har치 que el entrenamiento sea m치s r치pido)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consejos para mejores prestaciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo sencillo todo va r치pido, pero en casos reales puede que tengamos que esperar mucho tiempo a que el modelo se entrene. Para ello, podemos hacer algunas cosas para mejorar la velocidad de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5\n",
      "loss: 2.2985 [ 1000/60000]\n",
      "loss: 1.7096 [11000/60000]\n",
      "loss: 1.1138 [21000/60000]\n",
      "loss: 0.7948 [31000/60000]\n",
      "loss: 0.5966 [41000/60000]\n",
      "loss: 0.5211 [51000/60000]\n",
      "val_loss: 0.4289 val_acc: 0.8890\n",
      "epoch: 2/5\n",
      "loss: 0.4525 [ 1000/60000]\n",
      "loss: 0.3875 [11000/60000]\n",
      "loss: 0.3544 [21000/60000]\n",
      "loss: 0.3627 [31000/60000]\n",
      "loss: 0.3665 [41000/60000]\n",
      "loss: 0.3291 [51000/60000]\n",
      "val_loss: 0.3153 val_acc: 0.9127\n",
      "epoch: 3/5\n",
      "loss: 0.3497 [ 1000/60000]\n",
      "loss: 0.2919 [11000/60000]\n",
      "loss: 0.3682 [21000/60000]\n",
      "loss: 0.3421 [31000/60000]\n",
      "loss: 0.2746 [41000/60000]\n",
      "loss: 0.3036 [51000/60000]\n",
      "val_loss: 0.2749 val_acc: 0.9216\n",
      "epoch: 4/5\n",
      "loss: 0.2748 [ 1000/60000]\n",
      "loss: 0.2829 [11000/60000]\n",
      "loss: 0.2897 [21000/60000]\n",
      "loss: 0.2606 [31000/60000]\n",
      "loss: 0.2606 [41000/60000]\n",
      "loss: 0.2735 [51000/60000]\n",
      "val_loss: 0.2473 val_acc: 0.9303\n",
      "epoch: 5/5\n",
      "loss: 0.2378 [ 1000/60000]\n",
      "loss: 0.2458 [11000/60000]\n",
      "loss: 0.2263 [21000/60000]\n",
      "loss: 0.2305 [31000/60000]\n",
      "loss: 0.2421 [41000/60000]\n",
      "loss: 0.2043 [51000/60000]\n",
      "val_loss: 0.2263 val_acc: 0.9360\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    \"train\": Dataset(X[:60000], Y[:60000]), # 60.000 im치genes para entrenamiento\n",
    "    \"val\": Dataset(X[60000:], Y[60000:])    # 10.000 im치genes para validaci칩n\n",
    "}\n",
    "# aumentamos el tama침o del batch para aprovechar la GPU, carga en paralelo y movemos los datos a la GPU\n",
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=1000, shuffle=True, num_workers=4, pin_memory=True, collate_fn=dataset['train'].collate_fn),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=1000, num_workers=4, pin_memory=True, collate_fn=dataset['val'].collate_fn)\n",
    "}\n",
    "model = Model(784, 100, 10)\n",
    "# compilamos el modelo\n",
    "# model = torch.compile(model)\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "# torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "# movemos el modelo a la GPU\n",
    "model.cuda()                                                                   \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 5\n",
    "for e in range(1, epochs+1):\n",
    "    print(f\"epoch: {e}/{epochs}\")\n",
    "    # entrenamiento\n",
    "    model.train()\n",
    "    for batch_ix, (x, y) in enumerate(dataloader['train']):\n",
    "        x, y = x.cuda(), y.cuda()                                                   # movemos los datos a la GPU\n",
    "        optimizer.zero_grad()    \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):              # automatic mixed precision\n",
    "            outputs = model(x)              \n",
    "            loss = criterion(outputs, y)    \n",
    "        loss.backward()                 \n",
    "        optimizer.step()                \n",
    "        if batch_ix % 10 == 0:\n",
    "            loss, current = loss.item(), (batch_ix + 1) * len(x)\n",
    "            print(f\"loss: {loss:.4f} [{current:>5d}/{len(dataset['train']):>5d}]\")\n",
    "    # validaci칩n\n",
    "    model.eval()                \n",
    "    val_loss, val_acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_ix, (x, y) in enumerate(dataloader['val']):\n",
    "            x, y = x.cuda(), y.cuda()                                               # movemos los datos a la GPU\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):          # automatic mixed precision\n",
    "                outputs = model(x)              \n",
    "                loss = criterion(outputs, y)    \n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append((outputs.argmax(1) == y).float().mean().item())\n",
    "    print(f\"val_loss: {np.mean(val_loss):.4f} val_acc: {np.mean(val_acc):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de entrenar modelos muy grandes, es posible hacerlo en varias `GPUs` para acelerar el entrenamiento. Puedes aprender m치s sobre este tema [aqu칤](https://www.sensiocoders.com/blog/070_pytorch_distributed).\n",
    "\n",
    "Por 칰ltimo, te dejo tambi칠n mi [receta](https://www.sensiocoders.com/blog/033_receta_entrenamiento) de entrenamiento de redes neuronales, que puedes seguir a la hora de dise침ar y entrenar tus modelos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado tu modelo es posible que quieras exportarlo para usarlo en otro proyecto o incluso en producci칩n. Para ello, `Pytorch` nos ofrece diferents opciones para exportar nuestro modelo. La forma m치s sencilla es usar la funci칩n `torch.save`, que nos permite guardar el modelo en un fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez guardado lo podr치s leer desde cualquier otro proyecto usando la funci칩n `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = torch.load('model.pth')\n",
    "loaded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una alternativa m치s eficiente y flexible consiste en exporar el `state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "model = Model(784, 100, 10)\n",
    "model.load_state_dict(torch.load('model.ckpt'))\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem치s del modelo podemos guardar el `state_dict` del optimizador para poder continuar el entrenamiento en otro momento, por lo que es muy 칰til para guardar `checkpoints`.\n",
    "\n",
    "En ambos casos vas a necesitar el mismo c칩digo usado a la hora de exportar el modelo para poder cargarlo. Esto puede ser un inconveniente en muchos casos, por lo que una buena alternativa es usar `torch.script` para serializar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model) \n",
    "model_scripted.save('model_scripted.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Model\n",
       "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "  (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "  (fc2): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.jit.load('model_scripted.pt')\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta aproximaci칩n permite desacoplar el c칩digo de los pesos del modelo, sin embargo vamos a seguir necesitando `Pytorch` para poder cargar el modelo (lo cual no siempre ser치 posible en funci칩n del entorno de producci칩n del modelo). Para ello podemos usar `ONNX` para exportar el modelo a un formato est치ndar que puede ser usado por cualquier framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Diagnostic Run torch.onnx.export version 2.0.0.dev20230213+cu117 =======\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.10/site-packages/torch/onnx/utils.py:825: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f975d5d0540>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model.cpu(), \n",
    "    torch.randn(10, 784), \n",
    "    'model.onnx', \n",
    "    opset_version=11, \n",
    "    input_names=['input'], \n",
    "    output_names=['output'], \n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "\n",
    "onnx_model = onnx.load('model.onnx')\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `Python` podemos usar la librer칤a `onnxruntime` para cargar el modelo y usarlo en producci칩n, en los que no es necesario que `Pytorch` est칠 instalado (funciona directamente con `NumPy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession('model.onnx')\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: torch.randn(16, 784).numpy()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "ort_outs[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecosistema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` es un proyecto muy activo, con una gran comunidad de desarrolladores que contribuyen a su mejora. Adem치s, cuenta con una gran cantidad de librer칤as que nos permiten extender sus funcionalidades. Algunos ejemplos son:\n",
    "\n",
    "- `Torchvision`: librer칤a de visi칩n por computador para `Pytorch`, puedes aprender m치s sobre ella [aqu칤](https://www.sensiocoders.com/blog/055_torchvision).\n",
    "- `Torchtext`: librer칤a de procesamiento de lenguaje natural para `Pytorch`, puedes aprender m치s sobre ella [aqu칤](https://www.sensiocoders.com/blog/056_torchtext).\n",
    "- `Huggingface`: gran ecosistema conocido por su librer칤a de `transformers` pero que poco a poco a evolucionado incluyendo muchas otras funcionalidades, puedes aprender m치s sobre ella [aqu칤](https://www.sensiocoders.com/blog/105_hf_intro).\n",
    "- `Torhcmetrics`: librer칤a de m칠tricas para `Pytorch`.\n",
    "\n",
    "De entre todas ellas, destacar칤a [`Pytorch Lightning`](https://lightning.ai/docs/pytorch/stable/), una librer칤a que nos hace la vida m치s f치cil a la hora de entrenar modelos y que veremos en detalle en el siguiente post ya que tambi칠n acaba de lanzar su versi칩n 2.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
