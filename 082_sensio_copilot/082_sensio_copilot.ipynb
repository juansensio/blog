{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/082_sensio_copilot/082_sensio_copilot.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensio CoPilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a entrenar una red neuronal para generaci칩n de c칩digo similar al funcionamiento de [Github Copilot](https://copilot.github.com/). Llevo usando Copilot unos meses y la verdad que puedo decir que es una herramienta incre칤ble, que ha aumentado considerablemente mi productividad como programador. Hace unos d칤as le칤 [este](https://twitter.com/lvwerra/status/1467933794699259908?s=21) hilo en Twitter y cre칤 que ser칤a interesante intentar replicar un sistema como Copilot desde cero. As칤 que sin m치s dilaci칩n, 춰vamos a ello!\n",
    "\n",
    "Github Copilot utiliza el modelo conocido como [Codex](https://arxiv.org/abs/2107.03374), desarrollado por OpenAI. Este modelo est치 basado en la arquitectura GPT (de lo que hablaremos m치s adelante) y *tuneado* con c칩digo p칰blico extra칤do de Github. Esto significa que el modelo fue pre-entrenado de manera no supervisada con mucho texto y luego se hizo *fine tuning* con el c칩digo extra칤do de Github para la tarea de generaci칩n de texto (nuevo c칩digo autocompletado). En contraste, nosotros entrenaremos un modelo desde cero para la tarea de generaci칩n de texto con un dataset preparado para ello a modo de demostraci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset usado por OpenAI para entrenar Codex fue extra칤do de 54 millones de reposiotrios p칰blicos de Github, conteniendo 179 GB de archivos Python de menos de 1 MB. Tras varias etapas de procesado, el dataset final ocup칩 159 GB. Como no tenemos acceso a este dataset, usaremos [CodeParrot](https://huggingface.co/datasets/lvwerra/codeparrot-clean), un dataset elaborado por [HuggingFace](https://huggingface.co/) para la tarea de generaci칩n de c칩digo.\n",
    "\n",
    "> Recuerda que OpenAI al final tiene que ganar dinero de alguna manera, y 칠sta es cobrando por el uso de sus modelos a trav칠s de la API. Siendo el modelo p칰blico, su 칰nica ventaja competitiva reside en los datos usados durante el entrenamiento. Esto es una tendencia clara en el mundo del Software 2.0, d칩nde el valor real est치 en los datos y no en el c칩digo (aunque como dir칤a Andrej Karpathy en el Software 2.0 los datos SON el c칩digo y el modelo no es m치s que el binario resultante de la compilaci칩n del mismo, lo que llamamos el proceso de entrenamiento).\n",
    "\n",
    "El dataset ocupa unos 50 GB, aproximadamente una tercera parte del dataset usado originalmente por OpenAI. 춰Nada mal! Puedes descargarlo utilizando los siguientes comandos:\n",
    "\n",
    "```\n",
    "git clone https://huggingface.co/datasets/lvwerra/codeparrot-clean-train\n",
    "git clone https://huggingface.co/datasets/lvwerra/codeparrot-clean-valid\n",
    "```\n",
    "\n",
    "> Para poder descargarlos necesitar치s instalar [Git LFS](https://git-lfs.github.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52,\n",
       " ['data/codeparrot-clean-train/file-000000000007.json.gz',\n",
       "  'data/codeparrot-clean-train/file-000000000053.json.gz',\n",
       "  'data/codeparrot-clean-train/file-000000000026.json.gz'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "path = Path('data/codeparrot-clean-train')\n",
    "\n",
    "files = glob(str(path) + '/*.json.gz')\n",
    "len(files), files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>path</th>\n",
       "      <th>copies</th>\n",
       "      <th>size</th>\n",
       "      <th>content</th>\n",
       "      <th>license</th>\n",
       "      <th>hash</th>\n",
       "      <th>line_mean</th>\n",
       "      <th>line_max</th>\n",
       "      <th>alpha_frac</th>\n",
       "      <th>autogenerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jalavik/inspire-next</td>\n",
       "      <td>setup.py</td>\n",
       "      <td>1</td>\n",
       "      <td>4558</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n#\\n# This file is par...</td>\n",
       "      <td>gpl-2.0</td>\n",
       "      <td>-4849180608980663294</td>\n",
       "      <td>27.848101</td>\n",
       "      <td>77</td>\n",
       "      <td>0.615840</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dlzhangxg/cloud-ml-sdk</td>\n",
       "      <td>cloud_ml_samples/keras/mnist/trainer/task.py</td>\n",
       "      <td>1</td>\n",
       "      <td>2967</td>\n",
       "      <td># Copyright 2017 Xiaomi, Inc.\\n#\\n# Licensed u...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>-1822461891537938192</td>\n",
       "      <td>30.231579</td>\n",
       "      <td>74</td>\n",
       "      <td>0.649815</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openstack/heat</td>\n",
       "      <td>heat/engine/support.py</td>\n",
       "      <td>1</td>\n",
       "      <td>2683</td>\n",
       "      <td>#\\n#    Licensed under the Apache License, Ver...</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>-1815098437948811103</td>\n",
       "      <td>36.788732</td>\n",
       "      <td>78</td>\n",
       "      <td>0.622438</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imapp-pl/golem</td>\n",
       "      <td>tests/golem/network/test_golem_protocol.py</td>\n",
       "      <td>1</td>\n",
       "      <td>1444</td>\n",
       "      <td>import unittest\\nfrom devp2p.service import Wi...</td>\n",
       "      <td>gpl-3.0</td>\n",
       "      <td>-5671972220290336808</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>77</td>\n",
       "      <td>0.654432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>willimoa/pydal</td>\n",
       "      <td>pydal/dialects/mongo.py</td>\n",
       "      <td>1</td>\n",
       "      <td>22083</td>\n",
       "      <td>import re\\nfrom .._compat import PY2, basestri...</td>\n",
       "      <td>bsd-3-clause</td>\n",
       "      <td>7148857323817256703</td>\n",
       "      <td>34.389423</td>\n",
       "      <td>93</td>\n",
       "      <td>0.518951</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>georgestarcher/TA-SyncKVStore</td>\n",
       "      <td>bin/ta_synckvstore/cloudconnectlib/core/ext.py</td>\n",
       "      <td>1</td>\n",
       "      <td>10300</td>\n",
       "      <td>import calendar\\nimport json\\nimport re\\nimpor...</td>\n",
       "      <td>mit</td>\n",
       "      <td>-2116068727318744484</td>\n",
       "      <td>29.654762</td>\n",
       "      <td>80</td>\n",
       "      <td>0.593495</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>nabucosound/django-propaganda</td>\n",
       "      <td>propaganda/migrations/0001_initial.py</td>\n",
       "      <td>1</td>\n",
       "      <td>2828</td>\n",
       "      <td># -*- coding: utf-8 -*-\\nfrom __future__ impor...</td>\n",
       "      <td>bsd-3-clause</td>\n",
       "      <td>2285667758777388556</td>\n",
       "      <td>38.830986</td>\n",
       "      <td>114</td>\n",
       "      <td>0.541372</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>znuxor/aoc2016</td>\n",
       "      <td>4.py</td>\n",
       "      <td>1</td>\n",
       "      <td>41871</td>\n",
       "      <td>#!/usr/bin/python3\\nimport operator\\n\\n# room ...</td>\n",
       "      <td>bsd-3-clause</td>\n",
       "      <td>-2104829268388077325</td>\n",
       "      <td>40.662687</td>\n",
       "      <td>118</td>\n",
       "      <td>0.834659</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>sharadagarwal/autorest</td>\n",
       "      <td>AutoRest/Generators/Python/Python.Tests/Expect...</td>\n",
       "      <td>1</td>\n",
       "      <td>5365</td>\n",
       "      <td># coding=utf-8\\n# ----------------------------...</td>\n",
       "      <td>mit</td>\n",
       "      <td>-2646046330546511516</td>\n",
       "      <td>35.250000</td>\n",
       "      <td>110</td>\n",
       "      <td>0.630009</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>DeathSurvivorDE/dhbw_schreitbagger</td>\n",
       "      <td>Schreitbagger/Bagger_GUI_v0-0-0-3_st.py</td>\n",
       "      <td>1</td>\n",
       "      <td>8009</td>\n",
       "      <td>\\n'''\\nBagger_GUI v0.0.0.1\\n\\nGrafische Benutz...</td>\n",
       "      <td>gpl-3.0</td>\n",
       "      <td>54990144954214641</td>\n",
       "      <td>54.020690</td>\n",
       "      <td>212</td>\n",
       "      <td>0.507145</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows 칑 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                repo_name  \\\n",
       "0                    jalavik/inspire-next   \n",
       "1                  dlzhangxg/cloud-ml-sdk   \n",
       "2                          openstack/heat   \n",
       "3                          imapp-pl/golem   \n",
       "4                          willimoa/pydal   \n",
       "...                                   ...   \n",
       "99995       georgestarcher/TA-SyncKVStore   \n",
       "99996       nabucosound/django-propaganda   \n",
       "99997                      znuxor/aoc2016   \n",
       "99998              sharadagarwal/autorest   \n",
       "99999  DeathSurvivorDE/dhbw_schreitbagger   \n",
       "\n",
       "                                                    path  copies   size  \\\n",
       "0                                               setup.py       1   4558   \n",
       "1           cloud_ml_samples/keras/mnist/trainer/task.py       1   2967   \n",
       "2                                 heat/engine/support.py       1   2683   \n",
       "3             tests/golem/network/test_golem_protocol.py       1   1444   \n",
       "4                                pydal/dialects/mongo.py       1  22083   \n",
       "...                                                  ...     ...    ...   \n",
       "99995     bin/ta_synckvstore/cloudconnectlib/core/ext.py       1  10300   \n",
       "99996              propaganda/migrations/0001_initial.py       1   2828   \n",
       "99997                                               4.py       1  41871   \n",
       "99998  AutoRest/Generators/Python/Python.Tests/Expect...       1   5365   \n",
       "99999            Schreitbagger/Bagger_GUI_v0-0-0-3_st.py       1   8009   \n",
       "\n",
       "                                                 content       license  \\\n",
       "0      # -*- coding: utf-8 -*-\\n#\\n# This file is par...       gpl-2.0   \n",
       "1      # Copyright 2017 Xiaomi, Inc.\\n#\\n# Licensed u...    apache-2.0   \n",
       "2      #\\n#    Licensed under the Apache License, Ver...    apache-2.0   \n",
       "3      import unittest\\nfrom devp2p.service import Wi...       gpl-3.0   \n",
       "4      import re\\nfrom .._compat import PY2, basestri...  bsd-3-clause   \n",
       "...                                                  ...           ...   \n",
       "99995  import calendar\\nimport json\\nimport re\\nimpor...           mit   \n",
       "99996  # -*- coding: utf-8 -*-\\nfrom __future__ impor...  bsd-3-clause   \n",
       "99997  #!/usr/bin/python3\\nimport operator\\n\\n# room ...  bsd-3-clause   \n",
       "99998  # coding=utf-8\\n# ----------------------------...           mit   \n",
       "99999  \\n'''\\nBagger_GUI v0.0.0.1\\n\\nGrafische Benutz...       gpl-3.0   \n",
       "\n",
       "                      hash  line_mean  line_max  alpha_frac  autogenerated  \n",
       "0     -4849180608980663294  27.848101        77    0.615840          False  \n",
       "1     -1822461891537938192  30.231579        74    0.649815          False  \n",
       "2     -1815098437948811103  36.788732        78    0.622438          False  \n",
       "3     -5671972220290336808  37.000000        77    0.654432          False  \n",
       "4      7148857323817256703  34.389423        93    0.518951          False  \n",
       "...                    ...        ...       ...         ...            ...  \n",
       "99995 -2116068727318744484  29.654762        80    0.593495          False  \n",
       "99996  2285667758777388556  38.830986       114    0.541372          False  \n",
       "99997 -2104829268388077325  40.662687       118    0.834659          False  \n",
       "99998 -2646046330546511516  35.250000       110    0.630009          False  \n",
       "99999    54990144954214641  54.020690       212    0.507145          False  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "file = pd.read_json(files[2], lines=True)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!/usr/bin/env python\\n\\n# Copyright (C) 2014 Aldebaran Robotics\\n#\\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n#import ROS dependencies\\nimport rospy\\n\\n#import NAO dependencies\\nfrom naoqi_driver.naoqi_node import NaoqiNode\\nfrom geometry_msgs.msg import PoseStamped\\nfrom geometry_msgs.msg import Pose\\nimport almath\\nimport tf\\nfrom tf.transformations import euler_from_quaternion\\n\\nclass MoveToListener(NaoqiNode):\\n\\n    def __init__(self):\\n        NaoqiNode.__init__(self, \\'naoqi_moveto_listener\\')\\n        self.connectNaoQi()\\n        self.listener = tf.TransformListener()\\n\\n        self.subscriber = rospy.Subscriber(\"/move_base_simple/goal\", PoseStamped, self.callback)\\n\\n    # (re-) connect to NaoQI:\\n    def connectNaoQi(self):\\n        rospy.loginfo(\"Connecting to NaoQi at %s:%d\", self.pip, self.pport)\\n\\n        self.motionProxy = self.get_proxy(\"ALMotion\")\\n        if self.motionProxy is None:\\n            exit(1)\\n\\n    def callback(self, poseStamped):\\n        # reset timestamp because of bug: https://github.com/ros/geometry/issues/82\\n        poseStamped.header.stamp = rospy.Time(0)\\n        try:\\n            robotToTarget1 = self.listener.transformPose(\"/base_footprint\", poseStamped)\\n        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException) as e:\\n            rospy.logerr(\"Error while transforming pose: %s\", str(e))\\n            return\\n        quat = robotToTarget1.pose.orientation\\n        (roll,pitch,yaw) = euler_from_quaternion((quat.x, quat.y, quat.z, quat.w))\\n        self.motionProxy.moveTo(robotToTarget1.pose.position.x, robotToTarget1.pose.position.y, yaw)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.content[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En [posts](https://juansensio.com/blog/039_nlp_transfer) anteriores en los que ya hemos hablado sobre aplicaciones de NLP vimos que a una red neuronal no podemos darle texto como entrada, sino n칰meros. El proceso de convertir las palabras de nuestro dataset en n칰meros se conoce como `tokenizaci칩n`. Una opci칩n ser칤a construir un vector tan largo como n칰mero de palabras diferentes haya en el dataset y asignar un valor de 1 a la posici칩n determinada por la palabra en concreto. Esta forma de `tokenizaci칩n` se conoce como `one-hot-encoding` y como puedes imaginar es muy ineficiente. Lo m치s com칰n consiste en convertir cada palabra en n칰mero entero y luego darle a la red como entrada la fila correspondiente en una matriz de `embedding` con dimensionalidad fijada por nosotros. Esto es mucho m치s eficiente, ya que el vector ser치 denso y tambi칠n permite establecer relaciones matem치ticas entre palabras muy 칰til para la red. Si bien puedes implementar tu propia l칩gica de `tokenizaci칩n` aqu칤 usaremos una ya dado por la librer칤a `transformers`, de esta manera nos evitaremos muchos dolores de cabeza como diferenciar entre palabras en may칰scula o min칰scula, tratar los s칤mbolos, etc. En este caso usaremos el tokenizador usado por el modelo [BERT](https://arxiv.org/abs/1810.04805)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def', 'add', '_', 'numbers', '(', 'a', ',', 'b', ')', ':', 'return']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('def ADD_numbers(a, b):\\n return ')\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13366, 5587, 1035, 3616, 1006, 1037, 1010, 1038, 1007, 1024, 2709]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante tener en cuenta que el lenguaje usado en programaci칩n es mucho m치s restringido que el lenguaje natural, por lo que usar un `tokenizer` *custom* es probablemente una muy buena idea, reduciendo considerablemente el n칰mero de tokens necesarios y aumentando as칤 la eficiencia de nuestro modelo.\n",
    "\n",
    "> Puedes aprender m치s sobre `tokinizers` [aqu칤](https://huggingface.co/docs/transformers/tokenizer_summary). Como ver치s existen muchas aproximaciones al problema, desde soluciones tan simples como contar el n칰mero de ocurrencias de cada palabra y usar su posici칩n en la lista ordenada de todas las ocurrencias como token hasta reglas m치s complicadas. A medida que los transformers se hacen m치s grandes y potentes, pudiendo procesar longitdes m치s largas a la entrada, la 칰ltima moda es usar directamente la representaci칩n ASCII de los caracteres, evitando cualquier tipo de tokenizaci칩n 游땍 (de lo cual soy muy fan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 111, 108, 97]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'Hola'\n",
    "\n",
    "y = [ord(c) for c in x]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo usado por OpenAI, Codex, est치 basado en la arquitectura [GPT](https://paperswithcode.com/method/gpt) y contiene 12 billones de par치metros. Esto est치 un poco fuera de nuestra alcance (de momento 游땧) as칤 que usaremos la implementaci칩n de Karpathy, [minGPT](https://github.com/karpathy/minGPT), que nos permite entrenar peque침os transformers basados en la arquitectura GPT.\n",
    "\n",
    "`\n",
    "git clone https://github.com/karpathy/minGPT.git\n",
    "`\n",
    "\n",
    "Puedes jugar con el tama침os de la entrada (el n칰mero de token a usar), el n칰mero de capas, cabezas por capa y dimensi칩n interna de las capas de atenci칩n. Nuestro modelo recibir치 a la entrada un batch de trozos de c칩digo y nos dar치 a la salida un distribuci칩n de probabilidad sobre el vocabulario, d칩nde el valor m치s alto corresponder치 a aquella palabra que el modelo cree que ir치 a continuaci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(30522, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=30522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minGPT.mingpt.model import GPT, GPTConfig\n",
    "\n",
    "mconf = GPTConfig(tokenizer.vocab_size, block_size=512, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 30522])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor([indexes]).long()\n",
    "output, _ = model(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT implementa un tipo de mecanismo de atenci칩n *causal*, esto significa que cada palabra solo puede atender a las palabras anteriores. De esta manera evitamos que el modelo sea capaz de *ver en el futuro* durante el entrenamiento. En fase de inferencia, tendremos que predecir palabra a palabra, usando las predicciones como nuevos inputs para seguir prediciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def add _ numbers ( a , b ) : return invention shelf isaac'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minGPT.mingpt.utils import sample\n",
    "\n",
    "new_words = 3\n",
    "y = sample(model, x, new_words, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "y = tokenizer.convert_ids_to_tokens(y)\n",
    "' '.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos quedarnos con aquellas palabras con la mayor probabilidad o, lo que es m치s com칰n, samplear opciones siguiendo la distribuci칩n de probabilidad para generar diferentes posibilidades. Sin embargo, para que todo funciones, antes tendremos que entrenar nuestro modelo para que aprenda a generar c칩digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre os recomiendo, vamos a empezar haciendo el `fit` de una sola muestra para asegurarnos que todo est치 bien. Como los transformers trabajan con entradas de longitud fija vamos a a침adir el token `pad` para rellenar los huecos que haga falta. Adem치s, a침adiremos los tokens de inicio y final de frase para que el modelo sepa cuando terminar de predecir nuevas palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, examples):\n",
    "\t\tself.examples = examples \n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.examples)\n",
    "\n",
    "\tdef __getitem__(self, ix):\n",
    "\t\ttokens = tokenizer.tokenize(self.examples[ix])\n",
    "\t\ttokens = tokens[:max_input_length-2]\n",
    "\n",
    "\t\tinput = [tokenizer.cls_token_id] + \\\n",
    "\t\t\ttokenizer.convert_tokens_to_ids(tokens) + \\\n",
    "\t\t\t[tokenizer.pad_token_id] * (512 - len(tokens) - 1)\n",
    "\n",
    "\t\ttarget = tokenizer.convert_tokens_to_ids(tokens) + \\\n",
    "\t\t\t[tokenizer.sep_token_id] + \\\n",
    "\t\t\t[tokenizer.pad_token_id] * (512 - len(tokens) - 1)\n",
    "\n",
    "\t\tassert len(input) == 512\n",
    "\t\tassert len(target) == 512\n",
    "\n",
    "\t\treturn torch.LongTensor(input), torch.LongTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 13366,  5587,  1035,  3616,  1006,  1037,  1010,  1038,  1007,\n",
       "          1024,  2709,  1037,  1009,  1038,     0,     0]),\n",
       " tensor([13366,  5587,  1035,  3616,  1006,  1037,  1010,  1038,  1007,  1024,\n",
       "          2709,  1037,  1009,  1038,   102,     0,     0]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset(['def add_numbers(a, b):\\n return a + b'])\n",
    "\n",
    "inputs, target = ds[0]\n",
    "inputs[:17], target[:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `target` es igual que el `input` pero corrido un token a la derecha, as칤 para cada input el modelo intentar치 predecir la siguiente palabra. El primer token del input es el token de inicio de frase, para lo que el modelo nos dar치 la primera palabra. El 칰ltimo token del target es el token de final de frase, as칤 cuando el modelo reciba la 칰ltima palabra nos dar치 este token indicando que ya no debe predecir m치s. El token de `padding` sirve simplemente para tener una frase de longitud determinada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)\n",
    "\n",
    "x, y = next(iter(dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar, usaremos [Pytorch Lightning](https://www.pytorchlightning.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl  \n",
    "\n",
    "class Module(pl.LightningModule):\n",
    "\tdef __init__(self, model, tconf):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = model \n",
    "\t\tself.tconf = tconf\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)[0]\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\ty, loss = self.model(*batch)\n",
    "\t\treturn loss \n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\ty, loss = self.model(*batch)\n",
    "\t\tself.log('val_loss', loss.item())\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn self.model.configure_optimizers(self.tconf)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minGPT.mingpt.trainer import TrainerConfig\n",
    "\n",
    "mconf = GPTConfig(tokenizer.vocab_size, block_size=512, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "tconf = TrainerConfig(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95))\n",
    "\n",
    "module = Module(model, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 56.7 M\n",
      "-------------------------------\n",
      "56.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.7 M    Total params\n",
      "113.474   Total estimated model params size (MB)\n",
      "/home/juan/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:111: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f1fcb8264e46538ec610ffa659d48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tgpus=1,\n",
    "\tmax_epochs=100,\n",
    "\tprecision=16,\n",
    "\toverfit_batches=1,\n",
    "\tlogger=False,\n",
    "\tcheckpoint_callback=False,\n",
    ")\n",
    "\n",
    "trainer.fit(module, dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] def add _ numbers ( a , b ) : return a + b [SEP]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('def add_numbers(a, b):')\n",
    "indexes = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens) \n",
    "\n",
    "x = torch.tensor([indexes]).long()\n",
    "new_words = 5\n",
    "y = sample(model, x, new_words)[0]\n",
    "\n",
    "y = tokenizer.convert_ids_to_tokens(y)\n",
    "' '.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Genial! Nuestro modelo ha sido capaz de aprenderse de memoria una sola muestra. Ahora tenemos que entrenar con todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = Path('data/codeparrot-clean-train')\n",
    "val_path = Path('data/codeparrot-clean-valid')\n",
    "\n",
    "train_files = glob(str(train_path) + '/*.json.gz')\n",
    "val_files = glob(str(val_path) + '/*.json.gz')\n",
    "\n",
    "len(train_files), len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_json(train_files[0], lines=True) # le paso s칩lo un archivo, no se c칩mo hacer que los use todos 游쑆n",
    "val_file = pd.read_json(val_files[0], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 61373)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = {\n",
    "\t'train': Dataset(train_file.content.values),\n",
    "\t'val': Dataset(val_file.content.values),\n",
    "}\n",
    "\n",
    "len(ds['train']), len(ds['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = {\n",
    "\t'train': torch.utils.data.DataLoader(ds['train'], batch_size=32, shuffle=True, num_workers=20, pin_memory=True),\n",
    "\t'val': torch.utils.data.DataLoader(ds['val'], batch_size=32, shuffle=False, num_workers=20, pin_memory=True),\n",
    "}\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = GPTConfig(tokenizer.vocab_size, block_size=512, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)\n",
    "tconf = TrainerConfig(weight_decay=0.1, learning_rate=3e-4, betas=(0.9, 0.95))\n",
    "\n",
    "module = Module(model, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | GPT  | 56.7 M\n",
      "-------------------------------\n",
      "56.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "56.7 M    Total params\n",
      "113.474   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b768a0e949d149d1b49f40a0985e01b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5af2968585d4f73a2c7768413a8e62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b35c55226cc4df2944c50adf48bdb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed0445209b3470a9300a87dcea2e2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe89954c9b64c9787d11ac4db88c71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tgpus=1,\n",
    "\tmax_epochs=3,\n",
    "\tprecision=16,\n",
    "\tlimit_val_batches=100,\n",
    ")\n",
    "\n",
    "trainer.fit(module, dl['train'], dl['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] def add _ numbers ( a , b ) : return a + b [SEP]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('def add_numbers(a, b):')\n",
    "indexes = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens) \n",
    "\n",
    "x = torch.tensor([indexes]).long()\n",
    "new_words = 5\n",
    "y = sample(model, x, new_words)[0]\n",
    "\n",
    "y = tokenizer.convert_ids_to_tokens(y)\n",
    "' '.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "춰Voil치! Ya tenemos nuestro modelo GPT para generaci칩n de c칩gido 游댠 Para obtener un buen resultado tendr치s que entrenar m치s epochs, con todos los datos, un modelo m치s grande (puedes usar los papers de GPT2 y GPT3 para guiarte), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto c칩mo entrenar una red neuronal para generaci칩n de c칩digo Python inspir치ndonos en Github Copilot. Aqu칤 he utilizado diferentes librer칤as y herramientas, pero como comentaba al principio del post en [este](https://twitter.com/lvwerra/status/1467933794699259908?s=21) hilo encontrar치s c칩mo hacer lo mismo con las librer칤as de Huggingface, lo cu치l simplifica mucho todo y probablemete te permita conseguir muchos mejores resultados. En pr칩ximos posts entraremos m치s en detalle en el ecosistema de Huggingface. Te animo a que juegues un poco con el c칩digo, entrenando diferentes versiones del modelo. Para pasar al siguiente nivel, puedes exportar el modelo y desplegarlo en una API, hosteada en Heroku por ejemplo, y hacer una extensi칩n de VSCode que llame a la API para sugerir c칩digo directamente en el editor. Si lo haces, comp치rtelo en nuestro discord con el resto de la comunidad 游뱅"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74dbfc52f168b3071122cf9c0781887d6121c12f9c1b29bca56ce221bccb2a07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
