{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si has seguido la actividad de este blog hasta la fecha, deberías ser capaz de implementar y entrenar modelos sencillos de `Machine Learning` para diferentes tareas, como por ejemplo [redes neuronales convolucionales](https://sensioai.com/blog/042_cnns) para tareas de clasificación de imagen o [redes neuronales recurrentes](https://sensioai.com/blog/034_rnn_intro) para tareas de procesamiento de lenguaje. De ser así, es muy probable que en tu cabeza esté sonando una voz diciéndote: \"muy bien, ¿y ahora qué?\". Está muy bien ser capaz de llevar a cabo semejante tarea, no todo el mundo es capaz de ello y uno de los principales objetivos de este blog es el de aportar las herramientas necesarias a cualquiera para poder hacerlo. Sin embargo, de nada sirve entrenar un modelo si luego no lo vamos a **poner a trabajar**. Ésto significa dar acceso a nuestro modelo a cualquiera que quiera usarlo, por ejemplo a través de internet. Si, además, nuestro modelo soluciona un problema real por el que alguien esté dispuesto a darnos dinero, podremos incluso crear un negocio gracias a nuestras habilidades obtenidas a lo largo de los diferentes posts. Es precisamente de este tema del que empezamos a hablar en este post, y que seguiremos en los posteriores, cómo poner nuestras redes neuronales a trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entornos de Producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer aspecto que tenemos que tener en cuenta a la hora de poner nuestros modelos a trabajar es el entorno de producción en el que se van a encontrar. ¿Estarán en una aplicación móvil? En este caso podríamos tener acceso a la cámara o micrófono del aparato, además de otros sensores, para hacer aplicaciones personalizadas. Sin embargo, existen muchísimos tipos diferentes de *smartphones*, algunos más potentes que otros, pero en general con una capacidad computacional limitada, diferentes sistemas operativos (todos ellos diferentes a los que usamos para implementar y entrenar nuestros modelos), etc.\n",
    "\n",
    "![smartphone](data:image/jpeg;base64,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)\n",
    "\n",
    "¿Estará nuestro modelo alojado en servidor accesible a través de una aplicación web? En este caso tendremos mayor control sobre nuestra aplicación así como mucho más poder computacional a nuestro alcance, aunque a un mayor coste de mantenimiento. Además, deberemos tener en cuenta que los datos y resultados viajarán a través de internet imponiendo restricciones en la latencia y posibles problemas de privacidad. \n",
    "\n",
    "![web](https://us.123rf.com/450wm/simmmax/simmmax1506/simmmax150600065/41673257-analytics-and-programming-vector-web-application-optimization-responsive-pc.jpg?ver=6)\n",
    "\n",
    "¿Estará nuestro modelo funcionando en un tipo de *hardware* específico, como un coche o un robot? En este caso nuestro modelo deberá estar adaptado a la especificaciones de dicho *hardware*, lo cual implicará un esfuerzo extra de diseño pero con el potencial de poder llevar a cabo aplicaciones muy interesantes como, por ejemplo, conducción autónoma.\n",
    "\n",
    "![tesla](https://media1.giphy.com/media/H7rpSYHRyYgamxQNqw/giphy-downsized-large.gif)\n",
    "\n",
    "Como puedes ver, existen multitud de entornos de producción en el que pueden estar funcionando nuestros modelos, todos ellos muy diferentes a los entornos usados para la implementación y el entrenamiento de dichos algoritmos. Así pues, al estar desarrollando nuestros modelos, tenemos que ser conscientes del entrono en el que se van a ejecutar, lo cual impondrá ciertas restricciones que, de no tener en cuenta, solo resultarán en un fracaso total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sirviendo predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien hemos visto que existen multitud de entornos diferentes en los que pueden estar trabajando nuestros modelos, podemos distinguirlos en dos grande categorías: *on device* vs *cloud*.\n",
    "\n",
    "En el caso de las predicciones *on device*, nuestros modelos estarán trabajando directamente en el *hardware* del cliente. Puedes ser un *smarthpone* en el caso de una aplicación móvil, un ordenador portátil o de sobremesa en el caso de un programa o aplicación web en el que el modelo se ejecuta en el navegador web o directamente un coche para aplicaciones de conducción autónoma o un robot.\n",
    "\n",
    "| Pros | Contras |\n",
    "|------|---------|\n",
    "| Privacidad | Computación |\n",
    "| Latencia | Energía |\n",
    "| Coste | Monitoreo |\n",
    "| Flexibilidad | Software |\n",
    "\n",
    "Cuando nuestros modelos trabajan directamente en el *hardware* del cliente tendremos que lidiar con los requisitos computacionales disponibles, que pueden ser limitados. Además, en aquellos dispositivos que se alimentan con baterías, tendremos que tener en cuenta cuestiones energéticas ya que algunos modelos pueden necesitar mucha energía para llevar a cabo los cálculos necesarios. También tendremos que tener en cuenta que para ejecutar los modelos es posible que tengamos que adaptarlos o traducirlos para que funcionen con el software nativo del dispositivo (que puede ser diferente al usado para la implementación y entrenamiento del modelo). Por otro lado, disfrutaremos de una menor latencia ya que no tendremos que enviar datos y recibir predicciones, un mayor control sobre los costes, mejor privacidad y, sobre todo, una mayor flexibilidad en el tipo de aplicación.\n",
    "\n",
    "En el caso de las predicciones *cloud*, nuestro modelo estará ejecutándose en un servidor en la nube al cual se podrá conectar cualquier aplicación para recibir predicciones. En esta alternativa, las ventajas e inconvenientes mencionadas antes se invierten.\n",
    "\n",
    "| Contras | Pros |\n",
    "|------|---------|\n",
    "| Privacidad | Computación |\n",
    "| Latencia | Energía |\n",
    "| Coste | Monitoreo |\n",
    "| Flexibilidad | Software |\n",
    "\n",
    "También existe una opción intermedia conocida como *edge computaing*, utilizada en aplicaciones como *smart cities* o *smart factories* en las que se necesita mucho poder computacional con unos requisitos de latencia bajas. En este caso, se disponen servidores potentes cerca de los sensores, los cuales se conectan a éstos para enviar datos y generar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siriviendo predicciones en la nube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las diferentes alternativas, servir predicciones en la nube es la más sencilla para empezar ya que podremos utilizar nuestros modelos con pequeños cambios. En la siguiente imagen puedes ver diferentes alternativas que tenemos para poner nuestros modelos a trabajar en la nube.\n",
    "\n",
    "![](./cloud.png)\n",
    "\n",
    "La opción más simple es la de usar un servidor `HTTP`, en el caso de `Python` podemos usar la librería `Flask`. En este caso podremos integrar nuestro modelo y código en `Python` de manera muy sencilla y habilitar un `endpoint` al que poder enviar datos y recibir imágenes. Si bien es la opción más sencilla, es la menos escalable, por lo que sólo se aconseja usarla para pruebas de concepto. En el caso de necesitar soluciones más escalables, tenemos alternativas como utilizar servidores en Google Cloud, AWS o Azure con soluciones pre-configuradas (que dependerán de cada proveedor) o bien definiendo configuraciones desde cero en las que tener más control. Obviamente, en estos casos, la dificultad de uso aumenta de manera considerable. \n",
    "\n",
    "En este post vamos a ver un ejemplo sencillo de cómo definir un servidor usando `Flask` y habilitar su acceso a través de internet para recibir imágenes y devolver las etiquetas calculadas por un modelo de clasificación de imágenes sencillo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Flask](https://flask.palletsprojects.com/en/1.1.x/) es una librería de `Python` que nos facilita la vida a la hora de implementar un servidor web. Como su propio nombre indica, un servidor web es un programa que es capaz de \"servir\" datos a través de internet. Por ejemplo, cuando vas a google.com un servidor web te sirve la página web de google, un documento `html` que tu navegador interpreta renderizando todo el texto, imágenes y links que luego ves en la pantalla. En nuestro caso, utilizaremos un servidor para ejecutar nuestra red neuronal y devolver las predicciones calculadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Puedes instalar `Flask` con el comando `pip install flask`. Para hacer nuestro servidor trabajaremos con *scripts* de `Python`, para lo que necesitarás un editor de texto o un *IDE* (*integrated developement environment*). Aquí usaremos [VSCode](https://code.visualstudio.com/). Puedes aprender más sobre `Python`, instalación de librerías y cómo trabajar con *scripts* en este [post](https://sensioai.com/blog/001_python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask *hello world*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un vez instalada la librería, abre un nuevo documento (puedes llamarlo *hello.py*) y añade el siguiente código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T15:26:45.491320Z",
     "start_time": "2020-10-21T15:26:34.514443Z"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora ejecutas el *script*, `Flask` iniciará un servidor en tu máquina con una sola ruta (definida en la función `hello_world`), la cual nos devolverá el *string* `Hello, World!`.\n",
    "\n",
    "![](./code1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez iniciado el servidor, `Flask` habilita una `url` para podernos conectarnos al mismo. Por defecto, esta `url` es `http://127.0.0.1:5000/`. Si abres un navegador y visitas esta dirección, recibirás el resultado devuelto por la función `hello_world`.\n",
    "\n",
    "![](./flask1.png)\n",
    "\n",
    "Si bien esta es la forma más sencilla de conectarnos a nuestro servidor, nuestro objetivo es el de poder enviar datos al mismo para recibir luego los resultados de nuestro modelo. Para ello necesitaremos utilizar alguna herramienta que nos lo permita. Una muy útil es `curl`, un programa que nos permite conectarnos con servidores a través del terminal.\n",
    "\n",
    "![](./curl.png)\n",
    "\n",
    "Puedes probar ahora a añadir nuevos `endpoints`, definiendo nuevas funciones en el *script* con el decorador `@app.route('/endpoint')` donde puedes sustituir `endpoint` por la palabra que quieras usar para conectarte con el servidor. Además, estas rutas también aceptan parámetros para modificar su comportamiento o crear rutas dinámicas. Puedes aprender más sobre esto en la [documentación](https://flask.palletsprojects.com/en/1.1.x/quickstart/#quickstart) de `Flask`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desplegando en Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tenemos ya nuestro servidor funcionando, sin embargo si ahora intentas conectarte a la `url` desde otro ordenador verás que no recibes respuesta. Esto es debido a que tu servidor solo es visible para tu propia máquina (esto es lo que significan los números `127.0.0.1:5000`, `127.0.0.1` es tu `IP` local y `5000` es el puerto en el que está escuchando el servidor. Para poder acceder a nuestro servidor desde cualquier parte del mundo necesitamos una `url` similar a `midominio.com`, y para ello necesitaremos algún tipo de servicio de `hosting` en la nube. De entre los diferentes servicios que existen, uno fácil de utilizar y con buena integración con `Flask` es [Heroku](https://www.heroku.com/).\n",
    "\n",
    "Una vez creada tu cuenta (selecciona `Python` como el lenguaje principal) podrás crear una nueva aplicación desde el `dashboard`. \n",
    "\n",
    "![](./heroku1.png)\n",
    "\n",
    "Tras darle un nombre a tu aplicación, conéctala con un repositorio de Github. Para ello tendrás que crear un repositorio como el que puedes encontrar [aquí](https://github.com/sensioai/flask). Para que todo funcione bien necesitarás un par de archivos adicionales, además de tu *script* de `Python` con el servidor\n",
    "\n",
    "- `Procfile`: es un archivo especial que le indica a Heroku como arrancar el servidor. Puedes usar el mismo archivo que verás en el repositorio de ejemplo (simplemente asegúrate de llamar a tu *script* `app.py`).\n",
    "- `requirements.txt`: contiene las librerías de `Python` que requiere nuestro servidor para funcionar. Como mínimo necesitarás incluir `Flask`, `Flask-CORS` y `gunicorn`. Después, tendrás que incluir otras librerías como `Numpy`, `Pytorch` o cualquier otra que necesites.\n",
    "\n",
    "> La librería `Flask-CORS` es necesaria para poder utilizar nuestro servidor desde cualquier navegador sin problemas.\n",
    "\n",
    "![](./heroku2.png)\n",
    "\n",
    "y activa el *deployment* automático para que cada vez que hagamos cambios en el código y hagamos *push* de la rama principal, se actualice nuestra aplicación de manera automática.\n",
    "\n",
    "![](./heroku3.png)\n",
    "\n",
    "En la misma ventana, clica en el botón *Deploy Branch* para hacer el primer despliegue. Una vez termine el proceso, puedes abrir la aplicación con el botón *Open app*. Si todo va bien, aparecerá una nueva pestaña con la `url` de la aplicación que ahora es accesible desde cualquier parte del mundo ! Aquí puedes ver un ejemplo https://sensioai-blog-flask.herokuapp.com/\n",
    "\n",
    "> Si heroku no es capaz de reconocer tu aplicación de `Python`, asegúrate de tener seleccionado el *buildpack* de `Python` en los `Settings`.\n",
    "\n",
    "Ahora que ya sabemos cómo crear y desplegar nuestra aplicación sólo nos queda añadir nuestro modelo y habilitar una ruta para recibir imágenes y devolver las predicciones generadas por nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sirviendo predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este [notebook](https://colab.research.google.com/github/sensioai/blog/blob/master/051_gans/gans.ipynb) encontrarás el proceso completo para entrenar una red convolucional para clasificación de imágenes aéreas. Una vez ejecutado el notebook, encontrarás un archivo llamado `model.zip` que deberás adjuntar al código de tu aplicación `Flask`. En la siguiente implementación verás una nueva ruta llamada `predict` a la cual podremos enviar imágenes y recibir las predicciones de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from flask_cors import CORS\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "model = torch.jit.load('model.zip')\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route(\"/predict\", methods=['POST'])\n",
    "def predict():\n",
    "\n",
    "    # load image\n",
    "    img = Image.open(request.files['file'].stream).convert(\n",
    "        'RGB').resize((224, 224))\n",
    "    img = np.array(img)\n",
    "    img = torch.FloatTensor(img.transpose((2, 0, 1)) / 255)\n",
    "\n",
    "    # get predictions\n",
    "    preds = model(img.unsqueeze(0)).squeeze()\n",
    "    probas = torch.softmax(preds, axis=0)\n",
    "    ix = torch.argmax(probas, axis=0)\n",
    "\n",
    "    return {\n",
    "        'label': model.labels[ix],\n",
    "        'score': probas[ix].item()\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora actualizas la rama del repositorio de Github, también se actualizará la aplicación de Heroku de manera automática. asegúrate de actualizar también el archivo con las dependencias (`requirements.txt`) para incluir las nuevas librerías. Una vez completado el proceso, tenemos ya nuestro modelo disponible a través de la `url` proporcionada por Heroku. Podemos usar `curl` con ejemplos de imágenes del dataset utilizado para entrenar el modelo de la siguiente manera.\n",
    "\n",
    "![](./predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Puedes utilizar el [repositorio](https://github.com/sensioai/flask) de muestra como base para tu propia aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos empezado a ver las diferentes alternativas que encontramos a la hora de poner nuestros modelos a trabajar. En función de nuestra aplicación, estos algoritmos podrán ser ejecutados en los dispositivos de nuestros usuarios o bien en servidores en la nube controlados por nosotros mismos. Ambas alternativas presentan sus propias ventajas y desventajas en términos de privacidad, flexibilidad o poder computacional entre otros. También hemos veisto un ejemplo de cómo podemos desplegar un servidor en la nube para servir predicciones utilizando la librería `Flask` y `Heroku`. Si bien de esta manera hemos puesto a disposición de cualquier usuario nuestro modelo a través de internet, lo más común es interactuar con estas APIs a través de interfaces de usuario como, por ejemplo, aplicaciones web. En próximos posts veremos cómo podemos crear este tipo de interfaces y conectarlas con nuestro servidor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
